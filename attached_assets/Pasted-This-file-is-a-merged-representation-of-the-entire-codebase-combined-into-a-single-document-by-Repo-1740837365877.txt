This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.env.example
.gitattributes
.gitignore
cli.py
docker-compose.yml
README.md
scripts/install.ps1
scripts/install.sh
setup.py
src/calendar/google_calendar.py
src/config/config.py
src/ingestion/cloud/onedrive_client.py
src/ingestion/moodle/moodle_client.py
src/main.py
src/notifications/email_notifier.py
src/orchestration/workflow_manager.py
src/planner/study_planner.py
src/processing/content_processor.py
src/rag/knowledge_retriever.py
src/siyuan/siyuan_client.py
tests/test_ingestion.py
tests/test_processing.py

================================================================
Files
================================================================

================
File: .env.example
================
# General settings
APP_ENV=development
LOG_LEVEL=INFO

# Moodle credentials
MOODLE_URL=https://moodle.example.com
MOODLE_USERNAME=your_username
MOODLE_PASSWORD=your_password

# API Keys for external research
OPENAI_API_KEY=your_openai_api_key
PERPLEXITY_API_KEY=your_perplexity_api_key
OPEN_DEEP_RESEARCH_API_KEY=your_deep_research_api_key

# Google integration
GOOGLE_CLIENT_ID=your_google_client_id
GOOGLE_CLIENT_SECRET=your_google_client_secret
GOOGLE_CALENDAR_ID=your_calendar_id

# OneDrive integration
ONEDRIVE_CLIENT_ID=your_onedrive_client_id
ONEDRIVE_CLIENT_SECRET=your_onedrive_client_secret

# Path settings
LOCAL_STORAGE_PATH=C:/Users/pietr/OneDrive/Desktop/Progetti/StudyAssistant/data
OUTPUT_PATH=C:/Users/pietr/OneDrive/Desktop/Progetti/StudyAssistant/output

# Siyuan integration
SIYUAN_API_URL=http://localhost:6806
SIYUAN_TOKEN=your_siyuan_token

# Email notifications
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your_email@gmail.com
SMTP_PASSWORD=your_app_password
FROM_EMAIL=your_email@gmail.com
TO_EMAIL=your_email@gmail.com

# Services
REDIS_URL=redis://localhost:6379
RABBITMQ_URL=amqp://guest:guest@localhost:5672/

================
File: .gitattributes
================
# Auto detect text files and perform LF normalization
* text=auto

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Virtual environment
venv/
ENV/

# PyCharm, VSCode and other IDEs
.idea/
.vscode/
*.swp
*.swo

# Project specific
.env
output/
data/
*.log

# Tokens and credentials
*token*
*credential*
google_*.json

# Docker volume data
docker-data/

# n8n workflows
workflows/

# System files
.DS_Store
Thumbs.db

================
File: cli.py
================
#!/usr/bin/env python
import os
import sys
import argparse
import json
import datetime
from pathlib import Path

# Add src to path for imports
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

from src.main import (
    init_system, 
    ingest_moodle_materials, 
    ingest_onedrive_materials,
    ingest_local_materials,
    process_materials,
    generate_study_plan,
    import_to_siyuan,
    run_workflow
)

def display_material_summary(materials, title="Materials"):
    """Display a summary of materials."""
    print(f"\n=== {title} ===")
    if not materials:
        print("No materials found.")
        return
        
    for i, material in enumerate(materials, 1):
        print(f"{i}. {material.get('name', Path(material.get('path', 'unknown')).name)}")
        if 'status' in material:
            print(f"   Status: {material['status']}")
        if 'error' in material:
            print(f"   Error: {material['error']}")
    print(f"\nTotal: {len(materials)} items")

def interactive_mode():
    """Run the assistant in interactive mode."""
    print("\nWelcome to Smart Study & Wellbeing Assistant!")
    print("===========================================\n")
    
    # Initialize the system
    print("Initializing system...\n")
    init_system()
    
    while True:
        print("\nWhat would you like to do?")
        print("1. Ingest materials")
        print("2. Process materials")
        print("3. Generate study plan")
        print("4. Import to Siyuan")
        print("5. Run a workflow")
        print("0. Exit")
        
        choice = input("\nChoose an option (0-5): ").strip()
        
        if choice == "0":
            print("\nExiting Smart Study & Wellbeing Assistant. Goodbye!")
            break
        elif choice == "1":
            print("\n--- Ingest Materials ---")
            print("1. From Moodle")
            print("2. From OneDrive")
            print("3. From local directory")
            source_choice = input("\nChoose source (1-3): ").strip()
            
            if source_choice == "1":
                print("\nDownloading materials from Moodle...")
                courses = ingest_moodle_materials()
                print(f"\nDownloaded materials from {len(courses)} courses.")
            elif source_choice == "2":
                folder_path = input("\nEnter OneDrive folder path (default: /): ").strip() or "/"
                print(f"\nDownloading materials from OneDrive folder {folder_path}...")
                files = ingest_onedrive_materials(folder_path)
                display_material_summary(files, "OneDrive Files")
            elif source_choice == "3":
                directory = input("\nEnter local directory path: ").strip()
                if directory:
                    print(f"\nImporting materials from {directory}...")
                    files = ingest_local_materials(directory)
                    display_material_summary(files, "Local Files")
                else:
                    print("\nNo directory specified.")
            else:
                print("\nInvalid choice.")
                
        elif choice == "2":
            print("\n--- Process Materials ---")
            print("1. Moodle materials")
            print("2. External research materials")
            print("3. Personal materials")
            source_type_choice = input("\nChoose source type (1-3): ").strip()
            
            source_type = None
            if source_type_choice == "1":
                source_type = "moodle"
            elif source_type_choice == "2":
                source_type = "external_research"
            elif source_type_choice == "3":
                source_type = "personal"
                
            if source_type:
                path = input("\nEnter path to materials: ").strip()
                if path and Path(path).exists():
                    print(f"\nProcessing materials from {path}...")
                    materials = [{"path": str(file)} for file in Path(path).glob("**/*") if file.is_file()]
                    processed = process_materials(materials, source_type)
                    display_material_summary(processed, "Processed Materials")
                else:
                    print("\nInvalid path.")
            else:
                print("\nInvalid choice.")
                
        elif choice == "3":
            print("\n--- Generate Study Plan ---")
            course_name = input("\nEnter course name: ").strip()
            exam_date_str = input("\nEnter exam date (YYYY-MM-DD): ").strip()
            use_calendar = input("\nUse Google Calendar? (y/n): ").strip().lower() == 'y'
            
            if course_name and exam_date_str:
                try:
                    # Validate date format
                    datetime.date.fromisoformat(exam_date_str)
                    
                    print(f"\nGenerating study plan for {course_name}...")
                    result = generate_study_plan(course_name, exam_date_str, use_calendar)
                    
                    if result.get("status") == "success":
                        print(f"\nStudy plan generated successfully.")
                        print(f"Saved to: {result.get('plan_path')}")
                    else:
                        print(f"\nError: {result.get('error')}")
                except ValueError:
                    print("\nInvalid date format. Please use YYYY-MM-DD.")
            else:
                print("\nInvalid input.")
                
        elif choice == "4":
            print("\n--- Import to Siyuan ---")
            course_name = input("\nEnter course name: ").strip()
            path = input("\nEnter path to processed materials: ").strip()
            
            if course_name and path and Path(path).exists():
                print(f"\nImporting materials for {course_name} to Siyuan...")
                materials = []
                for file in Path(path).glob("**/*.md"):
                    materials.append({
                        "notes_path": str(file)
                    })
                    
                if materials:
                    success = import_to_siyuan(course_name, materials)
                    if success:
                        print("\nMaterials imported successfully to Siyuan.")
                    else:
                        print("\nFailed to import materials to Siyuan.")
                else:
                    print("\nNo markdown files found in the specified path.")
            else:
                print("\nInvalid input.")
                
        elif choice == "5":
            print("\n--- Run Workflow ---")
            workflow_name = input("\nEnter workflow name: ").strip()
            params_str = input("\nEnter parameters as JSON (or leave empty): ").strip()
            
            if workflow_name:
                params = {}
                if params_str:
                    try:
                        params = json.loads(params_str)
                    except json.JSONDecodeError:
                        print("\nInvalid JSON parameters. Using empty parameters.")
                        
                print(f"\nRunning workflow '{workflow_name}'...")
                result = run_workflow(workflow_name, params)
                print("\nWorkflow result:")
                print(json.dumps(result, indent=2))
            else:
                print("\nInvalid workflow name.")
                
        else:
            print("\nInvalid choice. Please choose a number from 0 to 5.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Smart Study & Wellbeing Assistant CLI")
    parser.add_argument("--interactive", "-i", action="store_true", help="Run in interactive mode")
    
    args = parser.parse_args()
    
    if args.interactive:
        interactive_mode()
    else:
        # Forward to main.py
        from src.main import main
        main()

================
File: docker-compose.yml
================
version: '3.8'

services:
  # Redis for caching and queuing
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes

  # RabbitMQ for message queuing
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"   # AMQP protocol
      - "15672:15672" # Management interface
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    restart: unless-stopped
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest

  # n8n for workflow automation
  n8n:
    image: n8nio/n8n
    ports:
      - "5678:5678"
    volumes:
      - n8n-data:/home/node/.n8n
    restart: unless-stopped
    environment:
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_HOST=localhost
      - NODE_ENV=production

  # Elasticsearch for logging
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.0
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    restart: unless-stopped

  # Kibana for log visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:7.10.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Grafana for monitoring
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped

volumes:
  redis-data:
  rabbitmq-data:
  n8n-data:
  elasticsearch-data:
  grafana-data:

================
File: README.md
================
# StudyAssistant

StudyAssistant offre uno strumento completo e personalizzato pensato per supportare lo studente universitario, integrando la gestione del materiale didattico e l'organizzazione degli studi con tecniche avanzate di apprendimento e benessere.

---

## 1. Visione e Obiettivi

L’obiettivo principale del progetto è fornire una piattaforma in grado di:

- **Acquisire Materiale Didattico:**  
  - Estrarre risorse da Moodle tramite *moodle-dl* (con fallback su *omniparser*).
  - Importare file da fonti personali (cartelle locali, OneDrive, Google Drive).

- **Organizzare e Arricchire i Contenuti:**  
  - Trasformare i dati in appunti, mappe concettuali, quiz e flashcard.
  - Integrare informazioni esterne tramite un motore di *Retrieval Augmented Generation (RAG)* solo se coerenti e pertinenti.
  - Ordinare le fonti utilizzate seguendo la priorità: Moodle > ricerca avanzata > materiale personale.

- **Generare Piani di Studio Personalizzati:**  
  - Creare piani di studio dinamici che considerino le date degli esami, gli impegni nel calendario e le tecniche di apprendimento (es. Pomodoro, spaced repetition).
  - Integrare attività di benessere, come pause, esercizio fisico e momenti di relax.

- **Fornire un’Interfaccia Intuitiva:**  
  - Utilizzare Siyuan per offrire un front-end flessibile e organizzato per la consultazione dei materiali.
  
- **Notificare e Monitorare:**  
  - Inviare notifiche email e sincronizzare con Google Calendar.
  - Utilizzare sistemi di caching, code di messaggi e strumenti di monitoraggio/logging (Grafana, Kibana) per garantire performance e scalabilità.

---

## 2. Architettura del Sistema

### 2.1 Componenti Principali

- **Modulo di Ingestione Dati:**  
  - Acquisizione dei file da Moodle (usando *moodle-dl* e *omniparser*) e da altre fonti (OneDrive, Google Drive, cartelle locali).
  - Gerarchia delle fonti: Moodle (più affidabile) > ricerca avanzata > materiale personale.

- **Modulo di Elaborazione Contenuti:**  
  - Conversione dei dati acquisiti in contenuti strutturati (appunti, mappe, quiz, flashcard).
  - Supporto al caching tramite Redis.

- **Motore di Ricerca Avanzata (RAG):**  
  - Integrazione e arricchimento dei contenuti con informazioni rilevanti da API esterne.
  - Riassunto delle fonti utilizzate, mantenendo la gerarchia di affidabilità.

- **Modulo Piani di Studio e Benessere:**  
  - Generazione di piani personalizzati che bilancino studio ed attività di benessere.
  - Sincronizzazione con date d’esame e impegni del calendario.

- **Interfaccia di Studio (Siyuan):**  
  - Front-end interattivo che organizza e visualizza i materiali e i piani di studio in modo intuitivo.

- **Modulo Notifiche e Pianificazione:**  
  - Invio automatico di email e sincronizzazione con Google Calendar per mantenere lo studente sempre aggiornato.

- **Modulo di Monitoraggio e Logging:**  
  - Utilizzo di Grafana e Kibana per monitorare le performance e gestire i log.

- **Orchestrazione e Code di Messaggi:**  
  - Coordinamento dei workflow tramite n8n e gestione delle richieste asincrone con RabbitMQ o Redis Streams.

### 2.2 Flusso dei Dati

1. **Acquisizione:**  
   - Scaricamento e normalizzazione dei dati da tutte le fonti, rispettando la gerarchia di affidabilità.

2. **Elaborazione e Arricchimento:**  
   - Conversione dei dati in contenuti strutturati e integrazione di informazioni esterne (RAG).

3. **Generazione del Piano di Studio:**  
   - Creazione di un piano personalizzato che combina sessioni di studio e momenti di benessere, aggiornandosi dinamicamente in base ai feedback.

4. **Output Organizzato:**  
   - Salvataggio dei materiali finali in cartelle per corso e inclusione di un riepilogo delle fonti utilizzate.

5. **Notifiche e Monitoraggio:**  
   - Invio di notifiche email e sincronizzazione con il calendario, uniti a sistemi di monitoraggio per garantire performance elevate.

---

## 3. Piano di Produzione

### Fase 1: Analisi e Progettazione (3 settimane)

- **Definizione dei Requisiti:**  
  Identificazione delle fonti dati e definizione della gerarchia delle fonti.  
- **Architettura:**  
  Progettazione dei moduli e dei flussi di lavoro mediante diagrammi.
- **Scelta degli Strumenti:**  
  Valutazione di *moodle-dl*, *omniparser*, n8n, Redis/RabbitMQ, Grafana/Kibana, Google Calendar, Siyuan e API di ricerca avanzata.

### Fase 2: Sviluppo del Prototipo (4-6 settimane)

- **Modulo Ingestione Dati:**  
  Sviluppo degli script per estrarre e importare i dati.
- **Modulo Elaborazione Contenuti:**  
  Integrazione dei modelli LLM per generare contenuti strutturati e utilizzo di Redis per il caching.
- **Motore RAG:**  
  Implementazione del modulo per l'arricchimento dei contenuti.
- **Modulo Piani di Studio:**  
  Creazione del sistema per generare piani di studio personalizzati.

### Fase 3: Integrazione e Test (3-4 settimane)

- **Orchestrazione:**  
  Collegamento dei vari moduli tramite n8n e gestione dei workflow asincroni.
- **Monitoraggio e Logging:**  
  Configurazione di Grafana e Kibana per il monitoraggio.
- **Test Completo:**  
  Verifica dell'intero sistema con dati reali.

### Fase 4: Deploy e Scalabilità (2 settimane)

- **Containerizzazione:**  
  Utilizzo di Docker per facilitare il deploy su VPS o cloud.
- **Output Organizzato:**  
  Configurazione delle cartelle di output per corso e inclusione dettagliata delle fonti.
- **Notifiche e Calendario:**  
  Finalizzazione dell’integrazione con Google Calendar e del sistema di notifiche.
- **Documentazione:**  
  Redazione di guide dettagliate per l’utilizzo e la manutenzione del sistema.

### Fase 5: Uso Personale e Evoluzione in Servizio (In Corso)

- **Test in Uso Personale:**  
  Utilizzo del sistema per raccogliere feedback ed ottimizzare le funzionalità.
- **Evoluzione a Servizio in Abbonamento:**  
  Sviluppo di funzionalità multi-tenant e premium (backup crittografati, supporto dedicato) per la trasformazione in un servizio a pagamento.

---

## 4. Conclusioni

StudyAssistant mira a combinare in modo innovativo gli strumenti di acquisizione dati, l’elaborazione intelligente dei contenuti e la gestione dinamica dello studio, garantendo al contempo uno stretto equilibrio con il benessere personale. Grazie a un approccio modulare e scalabile, il sistema è progettato per evolversi nel tempo, offrendo una soluzione sempre più completa e performante per il successo accademico.

================
File: scripts/install.ps1
================
# PowerShell script for installing on Windows

Write-Host "Installing Smart Study & Wellbeing Assistant..." -ForegroundColor Green

# Create virtual environment
Write-Host "Creating virtual environment..." -ForegroundColor Cyan
python -m venv venv

# Activate virtual environment
Write-Host "Activating virtual environment..." -ForegroundColor Cyan
& .\venv\Scripts\Activate.ps1

# Upgrade pip
Write-Host "Upgrading pip..." -ForegroundColor Cyan
python -m pip install --upgrade pip

# Install package in development mode
Write-Host "Installing package..." -ForegroundColor Cyan
pip install -e .

# Initialize project structure
Write-Host "Creating project structure..." -ForegroundColor Cyan
$dirs = @(
    "output\logs", 
    "output\processed", 
    "output\plans", 
    "output\moodle", 
    "output\onedrive", 
    "output\credentials"
)

foreach ($dir in $dirs) {
    if (-not (Test-Path $dir)) {
        New-Item -ItemType Directory -Path $dir -Force | Out-Null
    }
}

# Create sample .env file if not exists
if (-not (Test-Path .env)) {
    Write-Host "Creating sample .env file..." -ForegroundColor Cyan
    Copy-Item .env.example .env
    Write-Host "Please edit .env with your credentials" -ForegroundColor Yellow
}

# Initialize services using Docker if available
if (Get-Command docker -ErrorAction SilentlyContinue) {
    Write-Host "Docker detected. Starting services..." -ForegroundColor Cyan
    docker-compose up -d redis rabbitmq
    Write-Host "Redis and RabbitMQ services started." -ForegroundColor Green
}
else {
    Write-Host "Docker not detected. Please install Docker and Docker Compose to use the included services." -ForegroundColor Yellow
}

# Initialize the system
Write-Host "Initializing system..." -ForegroundColor Cyan
python -m src.main init

Write-Host ""
Write-Host "Installation complete!" -ForegroundColor Green
Write-Host "To use the assistant in interactive mode, run: python cli.py -i" -ForegroundColor Yellow
Write-Host ""

================
File: scripts/install.sh
================
#!/bin/bash

# Ensure script fails on error
set -e

echo "Installing Smart Study & Wellbeing Assistant..."

# Create virtual environment
echo "Creating virtual environment..."
python -m venv venv
source venv/bin/activate

# Upgrade pip
echo "Upgrading pip..."
pip install --upgrade pip

# Install package in development mode
echo "Installing package..."
pip install -e .

# Initialize project structure
echo "Creating project structure..."
mkdir -p output/logs output/processed output/plans output/moodle output/onedrive output/credentials

# Create sample .env file if not exists
if [ ! -f .env ]; then
  echo "Creating sample .env file..."
  cp .env.example .env
  echo "Please edit .env with your credentials"
fi

# Initialize services using Docker if available
if command -v docker >/dev/null 2>&1 && command -v docker-compose >/dev/null 2>&1; then
  echo "Docker detected. Starting services..."
  docker-compose up -d redis rabbitmq
  echo "Redis and RabbitMQ services started."
else
  echo "Docker not detected. Please install Docker and Docker Compose to use the included services."
fi

# Initialize the system
echo "Initializing system..."
python -m src.main init

echo ""
echo "Installation complete!"
echo "To use the assistant in interactive mode, run: python cli.py -i"
echo ""

================
File: setup.py
================
from setuptools import setup, find_packages

setup(
    name="smart-study-assistant",
    version="0.1.0",
    packages=find_packages(),
    include_package_data=True,
    install_requires=[
        "fastapi>=0.105.0",
        "pydantic>=2.5.2",
        "uvicorn>=0.24.0",
        "python-dotenv>=1.0.0",
        "requests>=2.31.0",
        "aiohttp>=3.9.1",
        "langchain>=0.0.350",
        "pandas>=2.1.3",
        "numpy>=1.26.2",
        "beautifulsoup4>=4.12.2",
        "moodle-dl>=2.3.0",
        "google-api-python-client>=2.108.0",
        "google-auth-oauthlib>=1.1.0",
        "msal>=1.25.0",
        "pika>=1.3.2",
        "redis>=5.0.1",
        "apscheduler>=3.10.4",
        "prometheus-client>=0.18.0",
        "python-json-logger>=2.0.7",
        "markdown>=3.5.1",
        "pytest>=7.4.3",
    ],
    entry_points={
        "console_scripts": [
            "studyassistant=cli:main",
        ],
    },
    python_requires=">=3.10",
    author="Your Name",
    author_email="your.email@example.com",
    description="A smart study assistant that integrates material processing with wellbeing planning",
    keywords="study, assistant, wellbeing, moodle, calendar",
    url="https://github.com/yourusername/smart-study-assistant",
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Education",
        "Topic :: Education",
        "Programming Language :: Python :: 3.10",
        "License :: OSI Approved :: MIT License",
    ],
)

================
File: src/calendar/google_calendar.py
================
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import datetime
import json
import pickle

from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build

from src.config.config import settings

logger = logging.getLogger(__name__)

class GoogleCalendar:
    """
    Integration with Google Calendar to synchronize study plans
    and retrieve existing commitments.
    """
    
    # Define the scopes for calendar access
    SCOPES = ['https://www.googleapis.com/auth/calendar']
    
    def __init__(self):
        self.credentials_dir = Path(settings.OUTPUT_PATH) / "credentials"
        self.credentials_dir.mkdir(parents=True, exist_ok=True)
        self.token_path = self.credentials_dir / "google_token.pickle"
        self.credentials_path = self.credentials_dir / "google_credentials.json"
        
        self.calendar_id = settings.GOOGLE_CALENDAR_ID or 'primary'
        self.service = None
        
    def _save_credentials(self, client_id: str, client_secret: str) -> bool:
        """
        Save Google API credentials to file.
        
        Args:
            client_id: Google client ID
            client_secret: Google client secret
            
        Returns:
            bool: True if saved successfully
        """
        try:
            credentials_data = {
                "installed": {
                    "client_id": client_id,
                    "client_secret": client_secret,
                    "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                    "token_uri": "https://oauth2.googleapis.com/token",
                    "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                    "redirect_uris": ["urn:ietf:wg:oauth:2.0:oob", "http://localhost"]
                }
            }
            
            with open(self.credentials_path, 'w') as f:
                json.dump(credentials_data, f)
                
            return True
            
        except Exception as e:
            logger.error(f"Error saving credentials: {e}")
            return False
    
    def authenticate(self) -> bool:
        """
        Authenticate with Google Calendar API.
        
        Returns:
            bool: True if authentication successful
        """
        try:
            credentials = None
            
            # Load existing token if available
            if self.token_path.exists():
                with open(self.token_path, 'rb') as token:
                    credentials = pickle.load(token)
            
            # Check if credentials are valid
            if credentials and credentials.valid:
                # Create the calendar service
                self.service = build('calendar', 'v3', credentials=credentials)
                logger.info("Using existing Google Calendar credentials")
                return True
                
            # If credentials expired but can be refreshed
            if credentials and credentials.expired and credentials.refresh_token:
                credentials.refresh(Request())
                # Save refreshed credentials
                with open(self.token_path, 'wb') as token:
                    pickle.dump(credentials, token)
                self.service = build('calendar', 'v3', credentials=credentials)
                logger.info("Refreshed Google Calendar credentials")
                return True
            
            # If no credentials, get them from the user
            if not self.credentials_path.exists():
                client_id = settings.GOOGLE_CLIENT_ID
                client_secret = settings.GOOGLE_CLIENT_SECRET
                
                if not client_id or not client_secret:
                    logger.error("Google client ID or secret not configured")
                    return False
                    
                self._save_credentials(client_id, client_secret)
            
            # Start OAuth flow
            flow = InstalledAppFlow.from_client_secrets_file(
                self.credentials_path, self.SCOPES
            )
            credentials = flow.run_local_server(port=0)
            
            # Save credentials
            with open(self.token_path, 'wb') as token:
                pickle.dump(credentials, token)
                
            # Create the calendar service
            self.service = build('calendar', 'v3', credentials=credentials)
            logger.info("New Google Calendar authentication successful")
            return True
            
        except Exception as e:
            logger.error(f"Error authenticating with Google Calendar: {e}")
            return False
    
    def get_events(self, start_date: datetime.date, end_date: datetime.date) -> List[Dict[str, Any]]:
        """
        Get events from Google Calendar for the given date range.
        
        Args:
            start_date: Start date
            end_date: End date
            
        Returns:
            List of events
        """
        if not self.service:
            if not self.authenticate():
                return []
                
        try:
            # Convert dates to RFC3339 format
            start_datetime = datetime.datetime.combine(start_date, datetime.time.min).isoformat() + 'Z'
            end_datetime = datetime.datetime.combine(end_date, datetime.time.max).isoformat() + 'Z'
            
            events_result = self.service.events().list(
                calendarId=self.calendar_id,
                timeMin=start_datetime,
                timeMax=end_datetime,
                singleEvents=True,
                orderBy='startTime'
            ).execute()
            
            events = events_result.get('items', [])
            
            # Process events to standardized format
            processed_events = []
            
            for event in events:
                # Skip declined events
                user_status = None
                for attendee in event.get('attendees', []):
                    if attendee.get('self', False):
                        user_status = attendee.get('responseStatus')
                        break
                        
                if user_status == 'declined':
                    continue
                    
                start = event.get('start', {})
                end = event.get('end', {})
                
                # Check if it's an all-day event
                all_day = 'date' in start and 'date' in end
                
                if all_day:
                    start_date_str = start.get('date')
                    end_date_str = end.get('date')
                    
                    # Convert to datetime for duration calculation
                    start_dt = datetime.datetime.fromisoformat(start_date_str)
                    # Subtract 1 day from end date as Google Calendar's end date is exclusive
                    end_dt = datetime.datetime.fromisoformat(end_date_str) - datetime.timedelta(days=1)
                    
                    duration_days = (end_dt - start_dt).days + 1
                    
                    processed_event = {
                        'id': event.get('id'),
                        'title': event.get('summary', 'No Title'),
                        'date': start_date_str,
                        'all_day': True,
                        'duration_days': duration_days
                    }
                else:
                    # Handle timed events
                    start_dt = datetime.datetime.fromisoformat(start.get('dateTime').replace('Z', '+00:00'))
                    end_dt = datetime.datetime.fromisoformat(end.get('dateTime').replace('Z', '+00:00'))
                    
                    duration_hours = (end_dt - start_dt).total_seconds() / 3600
                    
                    processed_event = {
                        'id': event.get('id'),
                        'title': event.get('summary', 'No Title'),
                        'date': start_dt.date().isoformat(),
                        'start_time': start_dt.time().isoformat(),
                        'end_time': end_dt.time().isoformat(),
                        'all_day': False,
                        'duration_hours': duration_hours
                    }
                
                # Add description and location if available
                if 'description' in event:
                    processed_event['description'] = event['description']
                    
                if 'location' in event:
                    processed_event['location'] = event['location']
                    
                processed_events.append(processed_event)
                
            return processed_events
            
        except Exception as e:
            logger.error(f"Error getting events from Google Calendar: {e}")
            return []
    
    def create_study_events(self, study_plan: Dict[str, Any]) -> bool:
        """
        Create events in Google Calendar based on a study plan.
        
        Args:
            study_plan: The study plan dictionary
            
        Returns:
            bool: True if events were created successfully
        """
        if not self.service:
            if not self.authenticate():
                return False
                
        try:
            course_name = study_plan.get("course_name", "Study Session")
            
            # Track success
            events_created = 0
            events_failed = 0
            
            # Create events for each day
            for day_plan in study_plan.get("daily_plans", []):
                date_str = day_plan.get("date")
                
                if not date_str:
                    continue
                    
                date_obj = datetime.date.fromisoformat(date_str)
                
                # Start time at 9 AM by default, adjust based on preferences
                start_hour = 9
                
                # Group sessions by type
                study_sessions = [s for s in day_plan.get("sessions", []) if s.get("type") == "study"]
                break_sessions = [s for s in day_plan.get("sessions", []) if s.get("type") == "break"]
                wellness_sessions = [s for s in day_plan.get("sessions", []) if s.get("type") == "wellness"]
                
                # Create one study event with breaks included in description
                if study_sessions:
                    total_study_minutes = sum(s.get("duration_minutes", 0) for s in study_sessions)
                    total_break_minutes = sum(b.get("duration_minutes", 0) for b in break_sessions)
                    
                    # Event start and end times
                    start_time = datetime.datetime.combine(date_obj, datetime.time(start_hour, 0))
                    end_time = start_time + datetime.timedelta(minutes=total_study_minutes + total_break_minutes)
                    
                    # Create description with pomodoro details
                    description = f"Study plan for {course_name}\n\n"
                    description += f"Total study time: {total_study_minutes} minutes\n"
                    description += f"Total break time: {total_break_minutes} minutes\n\n"
                    description += "Pomodoro sessions:\n"
                    
                    for i, (study, break_) in enumerate(zip(study_sessions, break_sessions + [{"duration_minutes": 0}])):
                        description += f"- Session {i+1}: {study.get('duration_minutes')} min study + {break_.get('duration_minutes')} min break\n"
                    
                    # Create the event
                    event = {
                        'summary': f"Study: {course_name}",
                        'location': '',
                        'description': description,
                        'start': {
                            'dateTime': start_time.isoformat(),
                            'timeZone': 'UTC',
                        },
                        'end': {
                            'dateTime': end_time.isoformat(),
                            'timeZone': 'UTC',
                        },
                        'reminders': {
                            'useDefault': False,
                            'overrides': [
                                {'method': 'popup', 'minutes': 15},
                            ],
                        },
                        'colorId': '1'  # Adjust color as needed
                    }
                    
                    try:
                        event = self.service.events().insert(calendarId=self.calendar_id, body=event).execute()
                        events_created += 1
                    except Exception as e:
                        logger.error(f"Error creating study event: {e}")
                        events_failed += 1
                
                # Create separate wellness events
                for wellness in wellness_sessions:
                    # Wellness events after study session
                    wellness_start = end_time if 'end_time' in locals() else datetime.datetime.combine(date_obj, datetime.time(start_hour + 2, 0))
                    wellness_end = wellness_start + datetime.timedelta(minutes=wellness.get("duration_minutes", 15))
                    
                    activity = wellness.get("activity", "wellness break").title()
                    
                    event = {
                        'summary': f"Wellness: {activity}",
                        'description': f"Take a {wellness.get('duration_minutes')} minute {activity} break to improve focus and well-being.",
                        'start': {
                            'dateTime': wellness_start.isoformat(),
                            'timeZone': 'UTC',
                        },
                        'end': {
                            'dateTime': wellness_end.isoformat(),
                            'timeZone': 'UTC',
                        },
                        'reminders': {
                            'useDefault': False,
                            'overrides': [
                                {'method': 'popup', 'minutes': 5},
                            ],
                        },
                        'colorId': '9'  # Different color for wellness
                    }
                    
                    try:
                        event = self.service.events().insert(calendarId=self.calendar_id, body=event).execute()
                        events_created += 1
                    except Exception as e:
                        logger.error(f"Error creating wellness event: {e}")
                        events_failed += 1
            
            logger.info(f"Created {events_created} events successfully, {events_failed} failed")
            return events_failed == 0
            
        except Exception as e:
            logger.error(f"Error creating study events in Google Calendar: {e}")
            return False

================
File: src/config/config.py
================
import os
from pathlib import Path
from typing import Dict, Any, Optional
from pydantic import BaseSettings, Field

class Settings(BaseSettings):
    # General settings
    APP_ENV: str = Field("development", env="APP_ENV")
    LOG_LEVEL: str = Field("INFO", env="LOG_LEVEL")
    
    # Moodle settings
    MOODLE_URL: str = Field(..., env="MOODLE_URL")
    MOODLE_USERNAME: str = Field(..., env="MOODLE_USERNAME")
    MOODLE_PASSWORD: str = Field(..., env="MOODLE_PASSWORD")
    
    # API Keys
    OPENAI_API_KEY: Optional[str] = Field(None, env="OPENAI_API_KEY")
    PERPLEXITY_API_KEY: Optional[str] = Field(None, env="PERPLEXITY_API_KEY")
    OPEN_DEEP_RESEARCH_API_KEY: Optional[str] = Field(None, env="OPEN_DEEP_RESEARCH_API_KEY")
    
    # Google settings
    GOOGLE_CLIENT_ID: Optional[str] = Field(None, env="GOOGLE_CLIENT_ID")
    GOOGLE_CLIENT_SECRET: Optional[str] = Field(None, env="GOOGLE_CLIENT_SECRET")
    GOOGLE_CALENDAR_ID: Optional[str] = Field(None, env="GOOGLE_CALENDAR_ID")
    
    # OneDrive settings
    ONEDRIVE_CLIENT_ID: Optional[str] = Field(None, env="ONEDRIVE_CLIENT_ID")
    ONEDRIVE_CLIENT_SECRET: Optional[str] = Field(None, env="ONEDRIVE_CLIENT_SECRET")
    
    # Path settings
    LOCAL_STORAGE_PATH: Path = Field(Path("data"), env="LOCAL_STORAGE_PATH")
    OUTPUT_PATH: Path = Field(Path("output"), env="OUTPUT_PATH")
    
    # Siyuan settings
    SIYUAN_API_URL: str = Field("http://localhost:6806", env="SIYUAN_API_URL")
    SIYUAN_TOKEN: Optional[str] = Field(None, env="SIYUAN_TOKEN")
    
    # Services settings
    REDIS_URL: str = Field("redis://localhost:6379", env="REDIS_URL")
    RABBITMQ_URL: str = Field("amqp://guest:guest@localhost:5672/", env="RABBITMQ_URL")
    
    # Source reliability hierarchy
    SOURCE_RELIABILITY: Dict[str, int] = {
        "moodle": 3,  # Highest priority
        "external_research": 2,
        "personal": 1  # Lowest priority
    }
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

settings = Settings()

================
File: src/ingestion/cloud/onedrive_client.py
================
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import msal
import requests
import json
import time

from src.config.config import settings

logger = logging.getLogger(__name__)

class OneDriveClient:
    """Client for interacting with OneDrive to download files and folders."""
    
    def __init__(self):
        self.client_id = settings.ONEDRIVE_CLIENT_ID
        self.client_secret = settings.ONEDRIVE_CLIENT_SECRET
        self.authority = "https://login.microsoftonline.com/common"
        self.scope = ["Files.Read", "Files.Read.All"]
        self.download_dir = Path(settings.OUTPUT_PATH) / "onedrive"
        self._ensure_dirs()
        self.token_cache_file = self.download_dir / "token_cache.json"
        self.token = None
        
    def _ensure_dirs(self):
        """Ensure necessary directories exist."""
        self.download_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_token_cache(self):
        """Get token cache from file or create new one."""
        if self.token_cache_file.exists():
            with open(self.token_cache_file, 'r') as f:
                return msal.SerializableTokenCache(json.load(f))
        return msal.SerializableTokenCache()
    
    def _save_token_cache(self, cache):
        """Save token cache to file."""
        with open(self.token_cache_file, 'w') as f:
            f.write(cache.serialize())
    
    def authenticate(self) -> bool:
        """
        Authenticate with OneDrive using MSAL.
        
        Returns:
            bool: True if authentication successful, False otherwise.
        """
        if not self.client_id or not self.client_secret:
            logger.error("OneDrive client ID or secret not configured")
            return False
            
        try:
            token_cache = self._get_token_cache()
            
            # Create MSAL app
            app = msal.ConfidentialClientApplication(
                self.client_id,
                authority=self.authority,
                client_credential=self.client_secret,
                token_cache=token_cache
            )
            
            # Check if token in cache
            accounts = app.get_accounts()
            if accounts:
                token_result = app.acquire_token_silent(self.scope, account=accounts[0])
                if token_result:
                    logger.info("Token acquired silently from cache")
                    self.token = token_result
                    self._save_token_cache(token_cache)
                    return True
            
            # If not in cache, initiate device code flow
            flow = app.initiate_device_flow(scopes=self.scope)
            
            if "user_code" not in flow:
                logger.error(f"Failed to initiate device flow: {flow.get('error')}")
                return False
                
            # Show instructions to user
            print(f"To authenticate with OneDrive, use the code {flow['user_code']} at {flow['verification_uri']}")
            
            # Poll for token
            token_result = app.acquire_token_by_device_flow(flow)
            
            if "access_token" not in token_result:
                logger.error(f"Failed to acquire token: {token_result.get('error')}")
                return False
                
            self.token = token_result
            self._save_token_cache(token_cache)
            logger.info("Successfully authenticated with OneDrive")
            return True
            
        except Exception as e:
            logger.error(f"Error authenticating with OneDrive: {e}")
            return False
    
    def list_files(self, folder_path: str = "/") -> List[Dict[str, Any]]:
        """
        List files and folders in the given OneDrive path.
        
        Args:
            folder_path: Path to list (default is root)
            
        Returns:
            List of file/folder info dictionaries
        """
        if not self.token:
            if not self.authenticate():
                return []
                
        try:
            # GraphAPI endpoint for drive items
            url = f"https://graph.microsoft.com/v1.0/me/drive/root:{folder_path}:/children"
            
            headers = {
                "Authorization": f"Bearer {self.token['access_token']}",
                "Content-Type": "application/json"
            }
            
            response = requests.get(url, headers=headers)
            
            if response.status_code != 200:
                logger.error(f"Failed to list files: {response.text}")
                # Token might be expired, try to refresh
                if self.authenticate():
                    return self.list_files(folder_path)
                return []
                
            data = response.json()
            return data.get("value", [])
            
        except Exception as e:
            logger.error(f"Error listing OneDrive files: {e}")
            return []
    
    def download_file(self, file_path: str, output_path: Optional[Path] = None) -> bool:
        """
        Download a file from OneDrive.
        
        Args:
            file_path: Path to the file in OneDrive
            output_path: Local path to save file (default uses original filename)
            
        Returns:
            bool: True if successful, False otherwise
        """
        if not self.token:
            if not self.authenticate():
                return False
                
        try:
            # GraphAPI endpoint for file download URL
            url = f"https://graph.microsoft.com/v1.0/me/drive/root:{file_path}:/content"
            
            headers = {
                "Authorization": f"Bearer {self.token['access_token']}"
            }
            
            response = requests.get(url, headers=headers, stream=True)
            
            if response.status_code != 200:
                logger.error(f"Failed to download file: {response.text}")
                # Token might be expired, try to refresh
                if self.authenticate():
                    return self.download_file(file_path, output_path)
                return False
            
            if output_path is None:
                filename = file_path.split('/')[-1]
                output_path = self.download_dir / filename
                
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
                        
            logger.info(f"Successfully downloaded {file_path} to {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"Error downloading file from OneDrive: {e}")
            return False

================
File: src/ingestion/moodle/moodle_client.py
================
import os
import subprocess
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
import requests

from src.config.config import settings

logger = logging.getLogger(__name__)

class MoodleClient:
    """
    Client for interacting with Moodle to download course materials.
    Uses moodle-dl as the primary tool with fallback to omniparser.
    """
    
    def __init__(self):
        self.moodle_url = settings.MOODLE_URL
        self.username = settings.MOODLE_USERNAME
        self.password = settings.MOODLE_PASSWORD
        self.download_dir = Path(settings.OUTPUT_PATH) / "moodle"
        self._ensure_dirs()
        
    def _ensure_dirs(self):
        """Ensure necessary directories exist."""
        self.download_dir.mkdir(parents=True, exist_ok=True)
    
    def download_with_moodle_dl(self) -> bool:
        """
        Download course materials using moodle-dl.
        
        Returns:
            bool: True if successful, False otherwise.
        """
        try:
            # Create moodle-dl config if it doesn't exist
            config_path = self.download_dir / "config.json"
            if not config_path.exists():
                config = {
                    "moodle_url": self.moodle_url,
                    "moodle_username": self.username,
                    "moodle_password": self.password,
                    "download_path": str(self.download_dir),
                    "download_course_ids": [],  # Empty means all enrolled courses
                    "download_linked_files": True,
                    "download_domains_whitelist": [],  # Empty means all domains
                }
                with open(config_path, 'w') as f:
                    json.dump(config, f, indent=4)
            
            # Run moodle-dl
            logger.info(f"Starting moodle-dl to download course materials to {self.download_dir}")
            result = subprocess.run(
                ["moodle-dl", "--path", str(self.download_dir)],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                logger.error(f"moodle-dl failed: {result.stderr}")
                return False
            
            logger.info(f"moodle-dl completed successfully: {result.stdout}")
            return True
        
        except Exception as e:
            logger.error(f"Error using moodle-dl: {e}")
            return False
    
    def download_with_omniparser(self) -> bool:
        """
        Fallback method to download course materials using custom omniparser approach.
        
        Returns:
            bool: True if successful, False otherwise.
        """
        try:
            session = requests.Session()
            
            # Login to Moodle
            login_url = f"{self.moodle_url}/login/index.php"
            response = session.get(login_url)
            
            # Extract login form token (this is simplified, actual implementation may need more parsing)
            login_token = "logintoken"  # This would be extracted from the form
            
            login_data = {
                "username": self.username,
                "password": self.password,
                "logintoken": login_token
            }
            
            response = session.post(login_url, data=login_data)
            
            if "login" in response.url.lower():
                logger.error("Login to Moodle failed using omniparser approach")
                return False
            
            # Get list of courses
            courses_url = f"{self.moodle_url}/my/"
            response = session.get(courses_url)
            
            # This would need a proper HTML parser to extract course IDs and links
            # For demonstration, we'll just log that we reached this point
            
            logger.info("Successfully accessed Moodle courses page with omniparser approach")
            logger.info("Implementation of course material extraction would be added here")
            
            return True
            
        except Exception as e:
            logger.error(f"Error using omniparser fallback: {e}")
            return False
    
    def download_course_materials(self) -> bool:
        """
        Download all course materials using moodle-dl with fallback to omniparser.
        
        Returns:
            bool: True if any method was successful, False if all failed.
        """
        success = self.download_with_moodle_dl()
        
        if not success:
            logger.warning("moodle-dl failed, trying omniparser fallback")
            success = self.download_with_omniparser()
        
        return success
    
    def get_available_courses(self) -> List[Dict[str, Any]]:
        """
        Get list of available courses.
        
        Returns:
            List[Dict[str, Any]]: List of course information dictionaries
        """
        courses_file = self.download_dir / "courses.json"
        
        if not courses_file.exists():
            logger.warning("Courses file not found. Run download_course_materials first.")
            return []
        
        try:
            with open(courses_file, 'r') as f:
                courses = json.load(f)
            return courses
        except Exception as e:
            logger.error(f"Error reading courses file: {e}")
            return []

================
File: src/main.py
================
import os
import sys
import logging
import argparse
from pathlib import Path
import json
import datetime

from src.config.config import settings
from src.ingestion.moodle.moodle_client import MoodleClient
from src.ingestion.cloud.onedrive_client import OneDriveClient
from src.processing.content_processor import ContentProcessor
from src.rag.knowledge_retriever import KnowledgeRetriever
from src.planner.study_planner import StudyPlanner
from src.siyuan.siyuan_client import SiyuanClient
from src.notifications.email_notifier import EmailNotifier
from src.calendar.google_calendar import GoogleCalendar
from src.orchestration.workflow_manager import WorkflowManager

# Set up logging
log_dir = Path(settings.OUTPUT_PATH) / "logs"
log_dir.mkdir(parents=True, exist_ok=True)

log_file = log_dir / f"studyassistant_{datetime.date.today().isoformat()}.log"

logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def init_system():
    """Initialize the system and check all components."""
    logger.info("Initializing Smart Study & Wellbeing Assistant")
    
    # Check paths
    logger.info(f"LOCAL_STORAGE_PATH: {settings.LOCAL_STORAGE_PATH}")
    logger.info(f"OUTPUT_PATH: {settings.OUTPUT_PATH}")
    
    # Ensure directories exist
    settings.LOCAL_STORAGE_PATH.mkdir(parents=True, exist_ok=True)
    settings.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
    
    # Check connections
    try:
        workflow_manager = WorkflowManager()
        logger.info("Workflow manager initialized")
    except Exception as e:
        logger.error(f"Error initializing workflow manager: {e}")
    
    # Test Moodle connection if credentials provided
    if all([settings.MOODLE_URL, settings.MOODLE_USERNAME, settings.MOODLE_PASSWORD]):
        try:
            moodle_client = MoodleClient()
            logger.info("Moodle client initialized")
        except Exception as e:
            logger.error(f"Error initializing Moodle client: {e}")
    else:
        logger.warning("Moodle credentials not configured")
    
    # Test Siyuan connection if URL provided
    if settings.SIYUAN_API_URL:
        try:
            siyuan_client = SiyuanClient()
            connection_ok = siyuan_client.check_connection()
            logger.info(f"Siyuan connection: {'OK' if connection_ok else 'Failed'}")
        except Exception as e:
            logger.error(f"Error initializing Siyuan client: {e}")
    else:
        logger.warning("Siyuan API URL not configured")
    
    logger.info("Initialization complete")

def ingest_moodle_materials():
    """Ingest materials from Moodle."""
    logger.info("Starting Moodle ingestion")
    
    moodle_client = MoodleClient()
    success = moodle_client.download_course_materials()
    
    if success:
        courses = moodle_client.get_available_courses()
        logger.info(f"Downloaded materials from {len(courses)} courses")
        return courses
    else:
        logger.error("Failed to download Moodle materials")
        return []

def ingest_onedrive_materials(folder_path: str = "/"):
    """Ingest materials from OneDrive."""
    logger.info(f"Starting OneDrive ingestion from {folder_path}")
    
    onedrive_client = OneDriveClient()
    if not onedrive_client.authenticate():
        logger.error("Failed to authenticate with OneDrive")
        return []
    
    files = onedrive_client.list_files(folder_path)
    logger.info(f"Found {len(files)} items in OneDrive folder '{folder_path}'")
    
    downloaded_files = []
    
    for file in files:
        # Skip folders for now
        if file.get("folder"):
            logger.info(f"Skipping folder: {file.get('name')}")
            continue
            
        # Download file
        file_path = f"{folder_path}/{file.get('name')}"
        if file_path.startswith("//"):
            file_path = file_path[1:]
            
        success = onedrive_client.download_file(file_path)
        
        if success:
            downloaded_files.append(file)
            logger.info(f"Downloaded file: {file.get('name')}")
        else:
            logger.error(f"Failed to download file: {file.get('name')}")
            
    return downloaded_files

def ingest_local_materials(directory: str = None):
    """Ingest materials from local directory."""
    if directory is None:
        directory = settings.LOCAL_STORAGE_PATH
    
    logger.info(f"Starting local ingestion from {directory}")
    
    directory_path = Path(directory)
    if not directory_path.exists():
        logger.error(f"Directory not found: {directory}")
        return []
    
    files = list(directory_path.glob("**/*"))
    regular_files = [f for f in files if f.is_file()]
    
    logger.info(f"Found {len(regular_files)} files in local directory")
    
    output_dir = Path(settings.OUTPUT_PATH) / "local"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    imported_files = []
    
    for file in regular_files:
        try:
            # Create relative path in output directory
            rel_path = file.relative_to(directory_path)
            dest_path = output_dir / rel_path
            
            # Create parent directories if needed
            dest_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Copy file
            import shutil
            shutil.copy2(file, dest_path)
            
            imported_files.append({
                "name": file.name,
                "path": str(dest_path),
                "size": file.stat().st_size,
                "modified": datetime.datetime.fromtimestamp(file.stat().st_mtime).isoformat()
            })
            
            logger.info(f"Imported local file: {file.name}")
            
        except Exception as e:
            logger.error(f"Error importing local file {file}: {e}")
    
    return imported_files

def process_materials(materials, source_type="moodle"):
    """Process materials into structured content."""
    logger.info(f"Processing {len(materials)} materials from {source_type}")
    
    processor = ContentProcessor()
    knowledge_retriever = KnowledgeRetriever()
    
    processed_materials = []
    
    for material in materials:
        try:
            file_path = Path(material.get("path"))
            if not file_path.exists():
                logger.warning(f"File not found: {file_path}")
                continue
                
            # Process the file
            content = processor.process_text_file(file_path, source_type)
            
            # Generate a search query for RAG based on the content
            query = f"Information about {file_path.stem} for university course"
            
            # Enrich with external knowledge if relevant
            if content.get("content", {}).get("summary"):
                try:
                    enriched_content = knowledge_retriever.enrich_content(content, query)
                    
                    # Generate source summary
                    source_summary = knowledge_retriever.generate_source_summary(enriched_content)
                    
                    # Create notes
                    notes_path = processor.generate_notes(enriched_content)
                    
                    processed_materials.append({
                        "original_file": str(file_path),
                        "notes_path": str(notes_path),
                        "source_summary": source_summary,
                        "source_type": source_type,
                        "status": "processed_and_enriched"
                    })
                    
                    logger.info(f"Processed and enriched: {file_path.name}")
                    
                except Exception as e:
                    logger.error(f"Error enriching content for {file_path}: {e}")
                    
                    # Fall back to basic processing
                    notes_path = processor.generate_notes(content)
                    
                    processed_materials.append({
                        "original_file": str(file_path),
                        "notes_path": str(notes_path),
                        "source_type": source_type,
                        "status": "processed_basic"
                    })
                    
                    logger.info(f"Processed (basic): {file_path.name}")
            else:
                # Process without enrichment
                notes_path = processor.generate_notes(content)
                
                processed_materials.append({
                    "original_file": str(file_path),
                    "notes_path": str(notes_path),
                    "source_type": source_type,
                    "status": "processed_basic"
                })
                
                logger.info(f"Processed (basic): {file_path.name}")
                
        except Exception as e:
            logger.error(f"Error processing material: {e}")
            processed_materials.append({
                "original_file": str(material.get("path", "unknown")),
                "error": str(e),
                "status": "error"
            })
    
    return processed_materials

def generate_study_plan(course_name, exam_date_str, use_calendar=True):
    """Generate a study plan for a course."""
    logger.info(f"Generating study plan for {course_name} with exam date {exam_date_str}")
    
    try:
        # Parse exam date
        exam_date = datetime.date.fromisoformat(exam_date_str)
        
        # Get existing commitments from calendar if requested
        existing_commitments = []
        if use_calendar:
            calendar = GoogleCalendar()
            if calendar.authenticate():
                # Get events from today to exam date
                today = datetime.date.today()
                events = calendar.get_events(today, exam_date)
                
                if events:
                    logger.info(f"Found {len(events)} calendar events to consider")
                    existing_commitments = events
                else:
                    logger.info("No calendar events found")
            else:
                logger.warning("Failed to authenticate with Google Calendar")
        
        # Create study plan
        planner = StudyPlanner()
        study_plan = planner.create_study_plan(
            course_name=course_name,
            exam_date=exam_date,
            study_hours_per_day=4.0,  # Default 4 hours per day
            existing_commitments=existing_commitments
        )
        
        # Save study plan
        plan_path = planner.save_study_plan(study_plan)
        logger.info(f"Study plan saved to {plan_path}")
        
        # Add to calendar if requested
        if use_calendar and calendar.service:
            logger.info("Adding study plan to Google Calendar")
            calendar.create_study_events(study_plan)
        
        # Send notification
        notifier = EmailNotifier()
        notifier.notify_study_plan(course_name, exam_date, plan_path)
        
        return {
            "status": "success",
            "plan_path": str(plan_path),
            "study_plan": study_plan
        }
        
    except Exception as e:
        logger.error(f"Error generating study plan: {e}")
        return {
            "status": "error",
            "error": str(e)
        }

def import_to_siyuan(course_name, materials):
    """Import materials to Siyuan notebook."""
    logger.info(f"Importing {len(materials)} materials for {course_name} to Siyuan")
    
    try:
        client = SiyuanClient()
        if not client.check_connection():
            logger.error("Failed to connect to Siyuan")
            return False
            
        # Format materials for import
        siyuan_materials = []
        
        for material in materials:
            # Read notes content
            notes_path = material.get("notes_path")
            if notes_path and Path(notes_path).exists():
                with open(notes_path, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
                    
                siyuan_materials.append({
                    "type": "notes",
                    "title": Path(notes_path).stem,
                    "markdown_content": markdown_content,
                    "source_summary": material.get("source_summary", "")
                })
        
        # Import to Siyuan
        notebook_id = client.import_course_materials(course_name, siyuan_materials)
        
        if notebook_id:
            logger.info(f"Successfully imported materials to Siyuan notebook {notebook_id}")
            return True
        else:
            logger.error("Failed to import materials to Siyuan")
            return False
            
    except Exception as e:
        logger.error(f"Error importing to Siyuan: {e}")
        return False

def run_workflow(name, params=None):
    """Run a workflow using the workflow manager."""
    if params is None:
        params = {}
        
    try:
        workflow_manager = WorkflowManager()
        result = workflow_manager.run_workflow(name, params)
        return result
    except Exception as e:
        logger.error(f"Error running workflow {name}: {e}")
        return {"status": "error", "message": str(e)}

def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Smart Study & Wellbeing Assistant")
    
    # Add subparsers for different commands
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    
    # Init command
    init_parser = subparsers.add_parser("init", help="Initialize the system")
    
    # Ingest command
    ingest_parser = subparsers.add_parser("ingest", help="Ingest materials")
    ingest_parser.add_argument("source", choices=["moodle", "onedrive", "local"], help="Source to ingest from")
    ingest_parser.add_argument("--path", help="Path for OneDrive or local source")
    
    # Process command
    process_parser = subparsers.add_parser("process", help="Process materials")
    process_parser.add_argument("source_type", choices=["moodle", "external_research", "personal"], 
                               help="Source type for reliability hierarchy")
    process_parser.add_argument("--path", required=True, help="Path to directory with materials")
    
    # Plan command
    plan_parser = subparsers.add_parser("plan", help="Generate study plan")
    plan_parser.add_argument("course", help="Course name")
    plan_parser.add_argument("exam_date", help="Exam date (YYYY-MM-DD)")
    plan_parser.add_argument("--no-calendar", action="store_true", help="Don't use Google Calendar")
    
    # Import command
    import_parser = subparsers.add_parser("import", help="Import to Siyuan")
    import_parser.add_argument("course", help="Course name")
    import_parser.add_argument("--path", required=True, help="Path to processed materials")
    
    # Workflow command
    workflow_parser = subparsers.add_parser("workflow", help="Run a workflow")
    workflow_parser.add_argument("name", help="Workflow name")
    workflow_parser.add_argument("--params", help="Workflow parameters as JSON")
    
    # Parse arguments
    args = parser.parse_args()
    
    # Execute command
    if args.command == "init":
        init_system()
    elif args.command == "ingest":
        if args.source == "moodle":
            ingest_moodle_materials()
        elif args.source == "onedrive":
            ingest_onedrive_materials(args.path or "/")
        elif args.source == "local":
            ingest_local_materials(args.path)
    elif args.command == "process":
        path = Path(args.path)
        if not path.exists():
            logger.error(f"Path not found: {path}")
            return
            
        materials = [{"path": str(file)} for file in path.glob("**/*") if file.is_file()]
        process_materials(materials, args.source_type)
    elif args.command == "plan":
        generate_study_plan(args.course, args.exam_date, not args.no_calendar)
    elif args.command == "import":
        path = Path(args.path)
        if not path.exists():
            logger.error(f"Path not found: {path}")
            return
            
        materials = []
        for file in path.glob("**/*.md"):
            materials.append({
                "notes_path": str(file)
            })
            
        import_to_siyuan(args.course, materials)
    elif args.command == "workflow":
        params = {}
        if args.params:
            try:
                params = json.loads(args.params)
            except json.JSONDecodeError:
                logger.error("Invalid JSON parameters")
                return
                
        result = run_workflow(args.name, params)
        print(json.dumps(result, indent=2))
    else:
        parser.print_help()

if __name__ == "__main__":
    main()

================
File: src/notifications/email_notifier.py
================
import os
import logging
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from pathlib import Path
from typing import List, Optional, Dict, Any
import datetime

from src.config.config import settings

logger = logging.getLogger(__name__)

class EmailNotifier:
    """
    Send email notifications for various events like new materials,
    upcoming study sessions, and wellness reminders.
    """
    
    def __init__(self):
        # Load email settings from environment
        self.smtp_server = os.environ.get("SMTP_SERVER", "smtp.gmail.com")
        self.smtp_port = int(os.environ.get("SMTP_PORT", 587))
        self.smtp_username = os.environ.get("SMTP_USERNAME", "")
        self.smtp_password = os.environ.get("SMTP_PASSWORD", "")
        self.from_email = os.environ.get("FROM_EMAIL", self.smtp_username)
        self.to_email = os.environ.get("TO_EMAIL", self.smtp_username)
        
    def _create_message(self, subject: str, body: str, html_body: Optional[str] = None) -> MIMEMultipart:
        """Create email message."""
        message = MIMEMultipart("alternative")
        message["Subject"] = subject
        message["From"] = self.from_email
        message["To"] = self.to_email
        
        # Attach plain text and HTML versions
        message.attach(MIMEText(body, "plain"))
        
        if html_body:
            message.attach(MIMEText(html_body, "html"))
            
        return message
        
    def send_email(self, subject: str, body: str, html_body: Optional[str] = None) -> bool:
        """
        Send an email notification.
        
        Args:
            subject: Email subject
            body: Plain text email body
            html_body: Optional HTML version of the email
            
        Returns:
            bool: True if email was sent successfully
        """
        if not all([self.smtp_server, self.smtp_username, self.smtp_password, self.to_email]):
            logger.warning("Email settings not configured, skipping notification")
            return False
            
        try:
            message = self._create_message(subject, body, html_body)
            
            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
                server.starttls()
                server.login(self.smtp_username, self.smtp_password)
                server.send_message(message)
                
            logger.info(f"Email notification sent: {subject}")
            return True
            
        except Exception as e:
            logger.error(f"Error sending email notification: {e}")
            return False
    
    def notify_new_materials(self, course_name: str, new_files: List[str]) -> bool:
        """
        Send notification about new course materials.
        
        Args:
            course_name: Name of the course
            new_files: List of new file names
            
        Returns:
            bool: True if notification was sent successfully
        """
        subject = f"New Materials for {course_name}"
        
        body = f"New materials available for {course_name}:\n\n"
        for file in new_files:
            body += f"- {file}\n"
            
        html_body = f"""
        <h2>New Materials for {course_name}</h2>
        <p>The following new materials are available:</p>
        <ul>
        """
        
        for file in new_files:
            html_body += f"<li>{file}</li>"
            
        html_body += "</ul>"
        
        return self.send_email(subject, body, html_body)
    
    def notify_study_plan(self, course_name: str, exam_date: datetime.date, study_plan_path: Path) -> bool:
        """
        Send notification about a new study plan.
        
        Args:
            course_name: Name of the course
            exam_date: Date of the exam
            study_plan_path: Path to the study plan file
            
        Returns:
            bool: True if notification was sent successfully
        """
        days_until_exam = (exam_date - datetime.date.today()).days
        
        subject = f"Study Plan for {course_name} - {days_until_exam} Days Until Exam"
        
        body = f"""
        Your study plan for {course_name} is ready!
        
        Exam Date: {exam_date}
        Days Until Exam: {days_until_exam}
        
        Your personalized study plan includes:
        - Daily schedules with Pomodoro sessions
        - Wellness activities to keep your mind and body healthy
        - Adjustments for your existing commitments
        
        The complete study plan is available at:
        {study_plan_path}
        """
        
        html_body = f"""
        <h2>Study Plan for {course_name}</h2>
        <p>Your personalized study plan is ready!</p>
        
        <p><strong>Exam Date:</strong> {exam_date}<br>
        <strong>Days Until Exam:</strong> {days_until_exam}</p>
        
        <p>Your plan includes:</p>
        <ul>
            <li>Daily schedules with Pomodoro sessions</li>
            <li>Wellness activities to keep your mind and body healthy</li>
            <li>Adjustments for your existing commitments</li>
        </ul>
        
        <p>The complete study plan is available at:<br>
        <a href="file://{study_plan_path}">{study_plan_path}</a></p>
        """
        
        return self.send_email(subject, body, html_body)
    
    def send_wellness_reminder(self, activity_type: str, duration: int) -> bool:
        """
        Send wellness activity reminder.
        
        Args:
            activity_type: Type of wellness activity
            duration: Duration in minutes
            
        Returns:
            bool: True if notification was sent successfully
        """
        subject = f"Wellness Break: Time for {activity_type.title()}"
        
        body = f"""
        It's time for a {duration}-minute wellness break!
        
        Activity: {activity_type.title()}
        
        Taking regular breaks improves focus, reduces stress, and enhances overall productivity.
        Enjoy your {activity_type} break!
        """
        
        html_body = f"""
        <h2>Wellness Break</h2>
        <p>It's time for a {duration}-minute wellness break!</p>
        
        <p><strong>Activity:</strong> {activity_type.title()}</p>
        
        <p>Taking regular breaks improves focus, reduces stress, and enhances overall productivity.
        Enjoy your {activity_type} break!</p>
        """
        
        return self.send_email(subject, body, html_body)

================
File: src/orchestration/workflow_manager.py
================
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Callable
import json
import time
import traceback
import pika
import redis

from src.config.config import settings

logger = logging.getLogger(__name__)

class WorkflowManager:
    """
    Orchestrates workflows using n8n or direct Python execution,
    with message queues for asynchronous processing.
    """
    
    def __init__(self):
        self.use_n8n = False  # Set to True when n8n integration is implemented
        self.workflows_dir = Path(settings.OUTPUT_PATH) / "workflows"
        self.workflows_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize message queue
        try:
            self.connection = pika.BlockingConnection(
                pika.URLParameters(settings.RABBITMQ_URL)
            )
            self.channel = self.connection.channel()
            self._setup_queues()
            self.use_rabbitmq = True
        except Exception as e:
            logger.warning(f"Could not connect to RabbitMQ: {e}. Using Redis for queues.")
            self.use_rabbitmq = False
            
            # Try Redis as fallback
            try:
                self.redis_client = redis.from_url(settings.REDIS_URL)
                logger.info("Using Redis for message queues")
            except Exception as e2:
                logger.error(f"Could not connect to Redis either: {e2}. Queue functionality disabled.")
        
    def _setup_queues(self):
        """Set up RabbitMQ queues."""
        if not self.use_rabbitmq:
            return
            
        # Declare queues with persistence
        queues = [
            "ingestion_queue",
            "processing_queue",
            "rag_queue",
            "planning_queue",
            "notification_queue",
            "siyuan_queue"
        ]
        
        for queue in queues:
            self.channel.queue_declare(
                queue=queue,
                durable=True,
                arguments={"x-message-ttl": 3600000}  # 1 hour TTL
            )
    
    def enqueue_task(self, queue_name: str, task_data: Dict[str, Any]) -> bool:
        """
        Add task to queue.
        
        Args:
            queue_name: Name of the queue
            task_data: Task data dictionary
            
        Returns:
            bool: True if successful
        """
        try:
            # Add timestamp
            task_data["enqueued_at"] = time.time()
            
            if self.use_rabbitmq:
                self.channel.basic_publish(
                    exchange='',
                    routing_key=queue_name,
                    body=json.dumps(task_data),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # persistent
                        content_type='application/json'
                    )
                )
                logger.info(f"Task added to RabbitMQ queue '{queue_name}'")
            else:
                # Use Redis list as queue
                self.redis_client.lpush(f"queue:{queue_name}", json.dumps(task_data))
                logger.info(f"Task added to Redis queue '{queue_name}'")
                
            return True
                
        except Exception as e:
            logger.error(f"Error enqueueing task: {e}")
            return False
    
    def run_workflow(self, workflow_name: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Run a workflow.
        
        Args:
            workflow_name: Name of the workflow
            params: Parameters for the workflow
            
        Returns:
            Result dictionary
        """
        if params is None:
            params = {}
            
        logger.info(f"Running workflow '{workflow_name}' with params {params}")
        
        # Save workflow execution details
        execution_id = f"{workflow_name}_{int(time.time())}"
        execution_path = self.workflows_dir / f"{execution_id}.json"
        
        with open(execution_path, 'w') as f:
            json.dump({
                "workflow": workflow_name,
                "params": params,
                "start_time": time.time(),
                "status": "running"
            }, f)
        
        result = {}
        
        try:
            if self.use_n8n:
                # Call n8n API to trigger workflow
                # This is a placeholder for actual n8n integration
                logger.info(f"Would trigger n8n workflow '{workflow_name}'")
                result = {"status": "success", "message": "n8n workflow triggered"}
            else:
                # Direct Python execution using workflow functions
                handler = getattr(self, f"_workflow_{workflow_name}", None)
                
                if handler:
                    result = handler(params)
                else:
                    result = {"status": "error", "message": f"Unknown workflow '{workflow_name}'"}
                    
            # Update execution record with result
            with open(execution_path, 'w') as f:
                json.dump({
                    "workflow": workflow_name,
                    "params": params,
                    "start_time": time.time(),
                    "end_time": time.time(),
                    "status": result.get("status", "unknown"),
                    "result": result
                }, f)
                
            return result
                
        except Exception as e:
            logger.error(f"Error executing workflow '{workflow_name}': {e}")
            
            # Update execution record with error
            with open(execution_path, 'w') as f:
                json.dump({
                    "workflow": workflow_name,
                    "params": params,
                    "start_time": time.time(),
                    "end_time": time.time(),
                    "status": "error",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                }, f)
                
            return {"status": "error", "message": str(e)}
    
    def process_queue(self, queue_name: str, handler: Callable[[Dict[str, Any]], Any], max_items: int = 10) -> int:
        """
        Process items from a queue.
        
        Args:
            queue_name: Name of the queue
            handler: Function to handle each item
            max_items: Maximum number of items to process
            
        Returns:
            Number of items processed
        """
        processed = 0
        
        try:
            if self.use_rabbitmq:
                # Process using RabbitMQ consume
                def callback(ch, method, properties, body):
                    nonlocal processed
                    if processed >= max_items:
                        return
                        
                    try:
                        data = json.loads(body)
                        handler(data)
                        ch.basic_ack(delivery_tag=method.delivery_tag)
                        processed += 1
                    except Exception as e:
                        logger.error(f"Error processing queue item: {e}")
                        # Negative acknowledgment, requeue message
                        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
                
                # Set prefetch count
                self.channel.basic_qos(prefetch_count=1)
                
                # Start consuming with a timeout
                self.channel.basic_consume(
                    queue=queue_name,
                    on_message_callback=callback
                )
                
                # Process messages until max_items is reached or timeout
                start_time = time.time()
                while processed < max_items and time.time() - start_time < 60:  # 60 second timeout
                    self.connection.process_data_events(time_limit=0.1)
                    if not self.channel.callbacks:
                        break
                
            else:
                # Process using Redis
                for _ in range(max_items):
                    # BRPOP blocks for up to 1 second
                    result = self.redis_client.brpop(f"queue:{queue_name}", 1)
                    
                    if result is None:
                        break
                        
                    _, item = result
                    try:
                        data = json.loads(item)
                        handler(data)
                        processed += 1
                    except Exception as e:
                        logger.error(f"Error processing Redis queue item: {e}")
                        # Put back in queue
                        self.redis_client.rpush(f"queue:{queue_name}", item)
                
            return processed
                
        except Exception as e:
            logger.error(f"Error processing queue '{queue_name}': {e}")
            return processed
            
    # Direct workflow implementations
    def _workflow_ingest_moodle(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Workflow to ingest Moodle content."""
        from src.ingestion.moodle.moodle_client import MoodleClient
        
        try:
            client = MoodleClient()
            success = client.download_course_materials()
            
            if success:
                courses = client.get_available_courses()
                return {
                    "status": "success",
                    "message": f"Downloaded materials from {len(courses)} courses",
                    "courses": courses
                }
            else:
                return {
                    "status": "error",
                    "message": "Failed to download course materials"
                }
                
        except Exception as e:
            logger.error(f"Error in ingest_moodle workflow: {e}")
            return {"status": "error", "message": str(e)}
    
    def _workflow_process_content(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Workflow to process content."""
        from src.processing.content_processor import ContentProcessor
        
        try:
            processor = ContentProcessor()
            file_path = params.get("file_path")
            source_type = params.get("source_type", "moodle")
            
            if not file_path:
                return {"status": "error", "message": "file_path is required"}
                
            file_path = Path(file_path)
            
            if not file_path.exists():
                return {"status": "error", "message": f"File not found: {file_path}"}
                
            content = processor.process_text_file(file_path, source_type)
            notes_path = processor.generate_notes(content)
            
            return {
                "status": "success",
                "message": f"Processed {file_path}",
                "notes_path": str(notes_path),
                "content": content
            }
                
        except Exception as e:
            logger.error(f"Error in process_content workflow: {e}")
            return {"status": "error", "message": str(e)}

================
File: src/planner/study_planner.py
================
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
import datetime
import random

from src.config.config import settings

logger = logging.getLogger(__name__)

class StudyPlanner:
    """
    Generate personalized study plans that incorporate exam dates,
    calendar commitments, and wellness activities.
    """
    
    def __init__(self):
        self.output_dir = Path(settings.OUTPUT_PATH) / "plans"
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def _generate_pomodoro_sessions(self, duration_hours: float) -> List[Dict[str, Any]]:
        """
        Generate Pomodoro sessions for a given study duration.
        
        Args:
            duration_hours: Total hours to study
            
        Returns:
            List of Pomodoro sessions
        """
        sessions = []
        
        # Standard Pomodoro: 25 min work, 5 min break, 15 min long break after 4 sessions
        pomodoro_count = int(duration_hours * 60 / 30)  # Approximate number of pomodoros
        
        for i in range(pomodoro_count):
            sessions.append({
                "type": "study",
                "duration_minutes": 25,
                "technique": "focused_work"
            })
            
            # Add break
            if (i + 1) % 4 == 0:
                sessions.append({
                    "type": "break",
                    "duration_minutes": 15,
                    "technique": "long_break"
                })
            else:
                sessions.append({
                    "type": "break",
                    "duration_minutes": 5,
                    "technique": "short_break"
                })
        
        return sessions
    
    def _generate_wellness_activities(self, count: int) -> List[Dict[str, Any]]:
        """
        Generate wellness activities.
        
        Args:
            count: Number of activities to generate
            
        Returns:
            List of wellness activities
        """
        activities = [
            {"type": "wellness", "activity": "meditation", "duration_minutes": 10},
            {"type": "wellness", "activity": "stretching", "duration_minutes": 15},
            {"type": "wellness", "activity": "walk", "duration_minutes": 20},
            {"type": "wellness", "activity": "hydration", "duration_minutes": 5},
            {"type": "wellness", "activity": "healthy_snack", "duration_minutes": 15},
            {"type": "wellness", "activity": "power_nap", "duration_minutes": 20},
            {"type": "wellness", "activity": "journaling", "duration_minutes": 15},
            {"type": "wellness", "activity": "deep_breathing", "duration_minutes": 5}
        ]
        
        return random.sample(activities, min(count, len(activities)))
    
    def create_study_plan(
        self,
        course_name: str,
        exam_date: datetime.date,
        study_hours_per_day: float = 4.0,
        existing_commitments: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a personalized study plan for a course.
        
        Args:
            course_name: Name of the course
            exam_date: Date of the exam
            study_hours_per_day: Hours to study per day (default 4)
            existing_commitments: List of existing commitments from calendar
            
        Returns:
            Study plan dictionary
        """
        if existing_commitments is None:
            existing_commitments = []
            
        today = datetime.date.today()
        days_until_exam = (exam_date - today).days
        
        if days_until_exam <= 0:
            logger.warning(f"Exam date {exam_date} is in the past or today")
            return {
                "error": "Exam date must be in the future",
                "course_name": course_name,
                "exam_date": str(exam_date)
            }
        
        # Create daily plans
        daily_plans = []
        
        for day_offset in range(1, days_until_exam + 1):
            current_date = today + datetime.timedelta(days=day_offset)
            
            # Skip days with full commitments
            day_commitments = [c for c in existing_commitments if c.get("date") == current_date.isoformat()]
            if any(c.get("all_day", False) for c in day_commitments):
                continue
                
            # Calculate available hours considering commitments
            committed_hours = sum(c.get("duration_hours", 0) for c in day_commitments)
            available_hours = max(0, study_hours_per_day - committed_hours)
            
            if available_hours < 1:
                continue  # Skip days with less than 1 hour available
                
            # Generate pomodoro sessions
            study_sessions = self._generate_pomodoro_sessions(available_hours * 0.8)  # 80% for study
            
            # Generate wellness activities (roughly 20% of the time)
            wellness_count = max(1, int(available_hours * 0.2 / 0.25))  # ~15min per activity
            wellness_activities = self._generate_wellness_activities(wellness_count)
            
            # Combine and organize sessions
            daily_plan = {
                "date": current_date.isoformat(),
                "available_hours": available_hours,
                "commitments": day_commitments,
                "sessions": study_sessions + wellness_activities
            }
            
            daily_plans.append(daily_plan)
        
        # Create overall plan
        study_plan = {
            "course_name": course_name,
            "exam_date": exam_date.isoformat(),
            "created_at": datetime.datetime.now().isoformat(),
            "days_until_exam": days_until_exam,
            "total_available_study_days": len(daily_plans),
            "total_study_hours": sum(day["available_hours"] for day in daily_plans),
            "daily_plans": daily_plans
        }
        
        return study_plan
    
    def save_study_plan(self, study_plan: Dict[str, Any], output_path: Optional[Path] = None) -> Path:
        """
        Save study plan to file.
        
        Args:
            study_plan: The study plan to save
            output_path: Path to save to (optional)
            
        Returns:
            Path where study plan was saved
        """
        if output_path is None:
            course_name = study_plan.get("course_name", "unknown").lower().replace(" ", "_")
            output_path = self.output_dir / f"{course_name}_study_plan.json"
            
        # Create JSON file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(study_plan, f, indent=2)
            
        logger.info(f"Study plan saved to {output_path}")
        
        # Create Markdown summary for easier reading
        md_path = output_path.with_suffix(".md")
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# Study Plan for {study_plan.get('course_name')}\n\n")
            f.write(f"**Exam Date:** {study_plan.get('exam_date')}\n")
            f.write(f"**Days Until Exam:** {study_plan.get('days_until_exam')}\n")
            f.write(f"**Total Study Hours:** {study_plan.get('total_study_hours')}\n\n")
            
            f.write("## Daily Schedule\n\n")
            
            for day in study_plan.get("daily_plans", []):
                f.write(f"### {day.get('date')}\n\n")
                f.write(f"Available Hours: {day.get('available_hours')}\n\n")
                
                if day.get("commitments"):
                    f.write("**Commitments:**\n\n")
                    for commitment in day.get("commitments", []):
                        f.write(f"- {commitment.get('title')}: {commitment.get('duration_hours')} hours\n")
                    f.write("\n")
                    
                f.write("**Sessions:**\n\n")
                
                for session in day.get("sessions", []):
                    if session.get("type") == "study":
                        f.write(f"- Study: {session.get('duration_minutes')} minutes ({session.get('technique')})\n")
                    elif session.get("type") == "break":
                        f.write(f"- Break: {session.get('duration_minutes')} minutes ({session.get('technique')})\n")
                    elif session.get("type") == "wellness":
                        f.write(f"- Wellness: {session.get('activity')}, {session.get('duration_minutes')} minutes\n")
                        
                f.write("\n")
        
        return output_path

================
File: src/processing/content_processor.py
================
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import json
import redis
import datetime

from src.config.config import settings

logger = logging.getLogger(__name__)

class ContentProcessor:
    """
    Process and transform content from various sources into structured formats
    like notes, concept maps, quizzes, and flashcards.
    """
    
    def __init__(self):
        self.output_dir = Path(settings.OUTPUT_PATH) / "processed"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize Redis for caching
        try:
            self.redis_client = redis.from_url(settings.REDIS_URL)
            self.use_cache = True
        except Exception as e:
            logger.warning(f"Could not connect to Redis: {e}. Caching disabled.")
            self.use_cache = False
    
    def _get_cache_key(self, file_path: Union[str, Path], content_type: str) -> str:
        """Generate a cache key for a file and content type."""
        if isinstance(file_path, Path):
            file_path = str(file_path)
        return f"content:{file_path}:{content_type}"
    
    def _get_from_cache(self, file_path: Union[str, Path], content_type: str) -> Optional[Dict[str, Any]]:
        """Get processed content from cache."""
        if not self.use_cache:
            return None
            
        cache_key = self._get_cache_key(file_path, content_type)
        cached_data = self.redis_client.get(cache_key)
        
        if cached_data:
            try:
                return json.loads(cached_data)
            except Exception as e:
                logger.warning(f"Failed to parse cached data: {e}")
                
        return None
    
    def _save_to_cache(self, file_path: Union[str, Path], content_type: str, content: Dict[str, Any], ttl: int = 86400) -> bool:
        """Save processed content to cache with TTL in seconds (default 1 day)."""
        if not self.use_cache:
            return False
            
        try:
            cache_key = self._get_cache_key(file_path, content_type)
            self.redis_client.setex(
                cache_key,
                ttl,
                json.dumps(content)
            )
            return True
        except Exception as e:
            logger.warning(f"Failed to cache content: {e}")
            return False
    
    def process_text_file(self, file_path: Path, source_type: str) -> Dict[str, Any]:
        """
        Process a text file into structured content.
        
        Args:
            file_path: Path to the file
            source_type: Type of source (moodle, external_research, personal)
            
        Returns:
            Dictionary with processed content
        """
        # Check cache first
        cached_content = self._get_from_cache(file_path, "notes")
        if cached_content:
            logger.info(f"Using cached content for {file_path}")
            return cached_content
        
        try:
            # Read the file content
            with open(file_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            
            # This would be replaced with actual LLM calls
            # For now, we'll just create a simple structure
            processed_content = {
                "original_file": str(file_path),
                "source_type": source_type,
                "reliability_score": settings.SOURCE_RELIABILITY.get(source_type, 0),
                "content": {
                    "summary": f"Summary of {file_path.name}",
                    "key_points": [
                        "Key point 1 from the document",
                        "Key point 2 from the document",
                        "Key point 3 from the document"
                    ],
                    "details": text_content[:500] + ("..." if len(text_content) > 500 else "")
                },
                "metadata": {
                    "processed_at": str(datetime.datetime.now()),
                    "word_count": len(text_content.split())
                }
            }
            
            # Save to cache
            self._save_to_cache(file_path, "notes", processed_content)
            
            return processed_content
            
        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            return {
                "error": str(e),
                "original_file": str(file_path),
                "source_type": source_type
            }
    
    def generate_notes(self, content: Dict[str, Any], output_path: Optional[Path] = None) -> Path:
        """
        Generate formatted notes from processed content.
        
        Args:
            content: Processed content dictionary
            output_path: Path to save the notes (optional)
            
        Returns:
            Path to the generated notes file
        """
        if output_path is None:
            filename = Path(content.get("original_file", "unknown")).stem
            output_path = self.output_dir / f"{filename}_notes.md"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(f"# Notes: {output_path.stem}\n\n")
                f.write(f"## Summary\n\n{content.get('content', {}).get('summary', '')}\n\n")
                
                f.write("## Key Points\n\n")
                for point in content.get('content', {}).get('key_points', []):
                    f.write(f"- {point}\n")
                
                f.write("\n## Details\n\n")
                f.write(content.get('content', {}).get('details', ''))
                
                f.write("\n\n## Source Information\n\n")
                f.write(f"- Source: {content.get('source_type', 'Unknown')}\n")
                f.write(f"- Reliability Score: {content.get('reliability_score', 0)}\n")
                f.write(f"- Original File: {content.get('original_file', 'Unknown')}\n")
            
            logger.info(f"Generated notes saved to {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Error generating notes: {e}")
            error_path = self.output_dir / "error_notes.md"
            with open(error_path, 'w', encoding='utf-8') as f:
                f.write(f"# Error Generating Notes\n\n{str(e)}")
            return error_path

================
File: src/rag/knowledge_retriever.py
================
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import requests
import json

from src.config.config import settings

logger = logging.getLogger(__name__)

class KnowledgeRetriever:
    """
    RAG (Retrieval Augmented Generation) system to enrich content
    with external information while maintaining source hierarchy.
    """
    
    def __init__(self):
        self.perplexity_api_key = settings.PERPLEXITY_API_KEY
        self.open_deep_research_api_key = settings.OPEN_DEEP_RESEARCH_API_KEY
        self.source_reliability = settings.SOURCE_RELIABILITY
        
    def _query_perplexity(self, query: str) -> Dict[str, Any]:
        """
        Query Perplexity API for additional information.
        
        Args:
            query: The search query
            
        Returns:
            Dictionary with results
        """
        if not self.perplexity_api_key:
            logger.warning("Perplexity API key not configured")
            return {"error": "API key not configured"}
            
        try:
            headers = {
                "Authorization": f"Bearer {self.perplexity_api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "query": query,
                "max_tokens": 1000
            }
            
            response = requests.post(
                "https://api.perplexity.ai/search",
                headers=headers,
                json=data
            )
            
            if response.status_code != 200:
                logger.error(f"Perplexity API error: {response.text}")
                return {"error": f"API error: {response.status_code}"}
                
            return response.json()
            
        except Exception as e:
            logger.error(f"Error querying Perplexity API: {e}")
            return {"error": str(e)}
    
    def _query_open_deep_research(self, query: str) -> Dict[str, Any]:
        """
        Query Open Deep Research API for additional information.
        
        Args:
            query: The search query
            
        Returns:
            Dictionary with results
        """
        if not self.open_deep_research_api_key:
            logger.warning("Open Deep Research API key not configured")
            return {"error": "API key not configured"}
            
        try:
            headers = {
                "Authorization": f"Bearer {self.open_deep_research_api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "query": query,
                "include_citations": True
            }
            
            response = requests.post(
                "https://api.opendeepresearch.org/v1/search",
                headers=headers,
                json=data
            )
            
            if response.status_code != 200:
                logger.error(f"Open Deep Research API error: {response.text}")
                return {"error": f"API error: {response.status_code}"}
                
            return response.json()
            
        except Exception as e:
            logger.error(f"Error querying Open Deep Research API: {e}")
            return {"error": str(e)}
    
    def enrich_content(self, content: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        Enrich content with external information.
        
        Args:
            content: Original processed content
            query: Search query for external sources
            
        Returns:
            Enriched content with external information and source details
        """
        # Try Perplexity first
        perplexity_results = self._query_perplexity(query)
        
        # Then Open Deep Research as backup
        if "error" in perplexity_results:
            deep_research_results = self._query_open_deep_research(query)
            external_results = deep_research_results
            source_name = "Open Deep Research"
        else:
            external_results = perplexity_results
            source_name = "Perplexity"
        
        # If both failed, return original content
        if "error" in external_results:
            logger.warning(f"External research failed: {external_results['error']}")
            return content
            
        # Create enriched content
        enriched_content = content.copy()
        
        # Add external research results
        if "content" not in enriched_content:
            enriched_content["content"] = {}
            
        # Extract the most relevant information from external results
        # This is a simplified version and would be more advanced in production
        external_info = {
            "source": source_name,
            "reliability_score": self.source_reliability.get("external_research", 0),
            "summary": external_results.get("summary", ""),
            "facts": external_results.get("facts", []),
            "citations": external_results.get("citations", [])
        }
        
        # Add external info to content
        if "external_research" not in enriched_content:
            enriched_content["external_research"] = []
            
        enriched_content["external_research"].append(external_info)
            
        return enriched_content
    
    def generate_source_summary(self, content: Dict[str, Any]) -> str:
        """
        Generate a summary of sources used in the content,
        following the hierarchy of reliability.
        
        Args:
            content: The enriched content with source information
            
        Returns:
            Formatted summary string of sources
        """
        sources = []
        
        # Add original source
        source_type = content.get("source_type")
        if source_type:
            reliability = self.source_reliability.get(source_type, 0)
            sources.append({
                "type": source_type,
                "name": content.get("original_file", "Unknown"),
                "reliability": reliability
            })
        
        # Add external research sources
        for ext in content.get("external_research", []):
            sources.append({
                "type": "external_research",
                "name": ext.get("source", "Unknown"),
                "reliability": ext.get("reliability_score", 0)
            })
            
        # Sort by reliability (highest first)
        sources.sort(key=lambda x: x["reliability"], reverse=True)
        
        # Generate summary
        summary = "# Source Summary\n\n"
        summary += "Sources are listed in order of reliability (highest first):\n\n"
        
        for i, source in enumerate(sources, 1):
            summary += f"{i}. **{source['name']}** (Type: {source['type']}, Reliability: {source['reliability']})\n"
            
            # Add citations if available
            if source["type"] == "external_research":
                for ext in content.get("external_research", []):
                    if ext.get("source") == source["name"]:
                        for j, citation in enumerate(ext.get("citations", []), 1):
                            summary += f"   - Citation {j}: {citation}\n"
                            
        return summary

================
File: src/siyuan/siyuan_client.py
================
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
import requests
import uuid

from src.config.config import settings

logger = logging.getLogger(__name__)

class SiyuanClient:
    """
    Client for interacting with Siyuan Note to organize and display
    processed study materials.
    """
    
    def __init__(self):
        self.api_url = settings.SIYUAN_API_URL
        self.token = settings.SIYUAN_TOKEN
        
    def _make_request(self, endpoint: str, method: str = "GET", data: Any = None) -> Dict[str, Any]:
        """Make API request to Siyuan."""
        url = f"{self.api_url}{endpoint}"
        headers = {
            "Content-Type": "application/json"
        }
        
        if self.token:
            headers["Authorization"] = f"Token {self.token}"
            
        try:
            if method.upper() == "GET":
                response = requests.get(url, headers=headers)
            elif method.upper() == "POST":
                response = requests.post(url, headers=headers, json=data)
            elif method.upper() == "PUT":
                response = requests.put(url, headers=headers, json=data)
            else:
                logger.error(f"Unsupported method: {method}")
                return {"code": -1, "msg": f"Unsupported method: {method}"}
                
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.HTTPError as e:
            logger.error(f"HTTP error: {e}")
            return {"code": -1, "msg": f"HTTP error: {e}"}
        except Exception as e:
            logger.error(f"Error making request to Siyuan: {e}")
            return {"code": -1, "msg": f"Error: {e}"}
    
    def check_connection(self) -> bool:
        """Check if connection to Siyuan API is working."""
        response = self._make_request("/api/system/currentTime")
        return response.get("code") == 0
    
    def create_notebook(self, name: str) -> Optional[str]:
        """
        Create a new notebook.
        
        Args:
            name: Name of the notebook
            
        Returns:
            Notebook ID if successful, None otherwise
        """
        response = self._make_request(
            "/api/notebook/createNotebook",
            "POST",
            {"name": name}
        )
        
        if response.get("code") == 0:
            # Get notebooks to find the ID of the newly created one
            notebooks = self.get_notebooks()
            for notebook in notebooks:
                if notebook.get("name") == name:
                    logger.info(f"Created notebook '{name}' with ID {notebook.get('id')}")
                    return notebook.get("id")
                    
        logger.error(f"Failed to create notebook '{name}': {response.get('msg')}")
        return None
    
    def get_notebooks(self) -> List[Dict[str, Any]]:
        """Get list of notebooks."""
        response = self._make_request("/api/notebook/lsNotebooks")
        
        if response.get("code") == 0:
            return response.get("data", [])
            
        logger.error(f"Failed to get notebooks: {response.get('msg')}")
        return []
    
    def create_document(self, notebook_id: str, path: str, content: str = "") -> Optional[str]:
        """
        Create a new document.
        
        Args:
            notebook_id: ID of the notebook
            path: Path within the notebook (e.g., "/folder/document")
            content: Markdown content
            
        Returns:
            Document ID if successful, None otherwise
        """
        # Ensure path starts with slash
        if not path.startswith('/'):
            path = '/' + path
            
        response = self._make_request(
            "/api/filetree/createDocWithMd",
            "POST",
            {
                "notebook": notebook_id,
                "path": path,
                "markdown": content
            }
        )
        
        if response.get("code") == 0:
            doc_id = response.get("data")
            logger.info(f"Created document at '{path}' with ID {doc_id}")
            return doc_id
            
        logger.error(f"Failed to create document: {response.get('msg')}")
        return None
    
    def update_document(self, document_id: str, content: str) -> bool:
        """
        Update document content.
        
        Args:
            document_id: ID of the document
            content: New markdown content
            
        Returns:
            True if successful, False otherwise
        """
        response = self._make_request(
            "/api/block/updateBlock",
            "POST",
            {
                "id": document_id,
                "data": content,
                "dataType": "markdown"
            }
        )
        
        success = response.get("code") == 0
        if success:
            logger.info(f"Updated document {document_id}")
        else:
            logger.error(f"Failed to update document: {response.get('msg')}")
            
        return success
    
    def import_course_materials(self, course_name: str, materials: List[Dict[str, Any]]) -> Optional[str]:
        """
        Import processed course materials into Siyuan.
        
        Args:
            course_name: Name of the course
            materials: List of processed materials
            
        Returns:
            Notebook ID if successful, None otherwise
        """
        # Check if notebook exists, create if not
        notebooks = self.get_notebooks()
        notebook_id = None
        
        for notebook in notebooks:
            if notebook.get("name") == course_name:
                notebook_id = notebook.get("id")
                break
                
        if not notebook_id:
            notebook_id = self.create_notebook(course_name)
            
        if not notebook_id:
            logger.error(f"Failed to create or find notebook for {course_name}")
            return None
            
        # Create index document
        index_content = f"# {course_name}\n\n"
        index_content += "## Course Materials\n\n"
        
        # Create documents for each material
        for material in materials:
            material_type = material.get("type", "notes")
            title = material.get("title", f"{material_type.title()} {str(uuid.uuid4())[:8]}")
            
            # Create path based on material type
            path = f"/{material_type}/{title}"
            
            # Get content
            if "markdown_content" in material:
                content = material["markdown_content"]
            else:
                # Create basic content based on material data
                content = f"# {title}\n\n"
                
                if "summary" in material:
                    content += f"## Summary\n\n{material['summary']}\n\n"
                    
                if "key_points" in material:
                    content += "## Key Points\n\n"
                    for point in material["key_points"]:
                        content += f"- {point}\n"
                    content += "\n"
                    
                if "details" in material:
                    content += f"## Details\n\n{material['details']}\n\n"
                    
                if "source" in material:
                    content += f"## Source\n\n{material['source']}\n\n"
            
            # Create the document
            doc_id = self.create_document(notebook_id, path, content)
            
            if doc_id:
                # Add link to index
                index_content += f"- [{title}](:id:{doc_id})\n"
        
        # Create source summary if available
        if any("source_summary" in m for m in materials):
            source_content = "# Source Summary\n\n"
            source_content += "Sources are listed in order of reliability (highest first):\n\n"
            
            for material in materials:
                if "source_summary" in material:
                    source_content += material["source_summary"] + "\n\n"
                    
            self.create_document(notebook_id, "/sources/Source Summary", source_content)
            index_content += "\n## [Source Summary](:id:{doc_id})\n"
        
        # Update index document
        self.create_document(notebook_id, "/index", index_content)
        
        return notebook_id

================
File: tests/test_ingestion.py
================
import os
import sys
import unittest
from pathlib import Path
from unittest.mock import patch, MagicMock
import json
import tempfile

# Add project root to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.ingestion.moodle.moodle_client import MoodleClient
from src.ingestion.cloud.onedrive_client import OneDriveClient

class TestMoodleClient(unittest.TestCase):
    
    def setUp(self):
        # Create test output directory
        self.test_output_dir = tempfile.mkdtemp()
        self.patcher = patch('src.config.config.settings')
        self.mock_settings = self.patcher.start()
        self.mock_settings.OUTPUT_PATH = Path(self.test_output_dir)
        self.mock_settings.MOODLE_URL = "https://moodle.example.com"
        self.mock_settings.MOODLE_USERNAME = "test_user"
        self.mock_settings.MOODLE_PASSWORD = "test_password"
        
    def tearDown(self):
        # Clean up
        self.patcher.stop()
        
    @patch('subprocess.run')
    def test_download_with_moodle_dl_success(self, mock_run):
        # Mock successful execution of moodle-dl
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "Download completed successfully"
        mock_run.return_value = mock_process
        
        client = MoodleClient()
        result = client.download_with_moodle_dl()
        
        self.assertTrue(result)
        mock_run.assert_called_once()
        
    @patch('subprocess.run')
    def test_download_with_moodle_dl_failure(self, mock_run):
        # Mock failed execution of moodle-dl
        mock_process = MagicMock()
        mock_process.returncode = 1
        mock_process.stderr = "Error: Connection failed"
        mock_run.return_value = mock_process
        
        client = MoodleClient()
        result = client.download_with_moodle_dl()
        
        self.assertFalse(result)
        mock_run.assert_called_once()
        
    @patch('requests.Session')
    def test_download_with_omniparser_success(self, mock_session):
        # Mock successful connection to Moodle
        mock_session_instance = MagicMock()
        mock_response = MagicMock()
        mock_response.url = "https://moodle.example.com/my/"
        mock_session_instance.post.return_value = mock_response
        mock_session_instance.get.return_value = mock_response
        mock_session.return_value = mock_session_instance
        
        client = MoodleClient()
        result = client.download_with_omniparser()
        
        self.assertTrue(result)
        
    @patch('requests.Session')
    def test_download_with_omniparser_failed_login(self, mock_session):
        # Mock failed login to Moodle
        mock_session_instance = MagicMock()
        mock_response = MagicMock()
        mock_response.url = "https://moodle.example.com/login/index.php"
        mock_session_instance.post.return_value = mock_response
        mock_session_instance.get.return_value = mock_response
        mock_session.return_value = mock_session_instance
        
        client = MoodleClient()
        result = client.download_with_omniparser()
        
        self.assertFalse(result)
        
    def test_get_available_courses_no_file(self):
        # Test getting courses when courses.json doesn't exist
        client = MoodleClient()
        courses = client.get_available_courses()
        
        self.assertEqual(courses, [])
        
    def test_get_available_courses_with_file(self):
        # Create a mock courses.json file
        client = MoodleClient()
        courses_file = client.download_dir / "courses.json"
        courses_file.parent.mkdir(parents=True, exist_ok=True)
        
        test_courses = [
            {"id": 1, "name": "Mathematics 101"},
            {"id": 2, "name": "Physics 201"}
        ]
        
        with open(courses_file, 'w') as f:
            json.dump(test_courses, f)
            
        # Test getting courses
        courses = client.get_available_courses()
        
        self.assertEqual(courses, test_courses)

class TestOneDriveClient(unittest.TestCase):
    
    def setUp(self):
        # Create test output directory
        self.test_output_dir = tempfile.mkdtemp()
        self.patcher = patch('src.config.config.settings')
        self.mock_settings = self.patcher.start()
        self.mock_settings.OUTPUT_PATH = Path(self.test_output_dir)
        self.mock_settings.ONEDRIVE_CLIENT_ID = "test_client_id"
        self.mock_settings.ONEDRIVE_CLIENT_SECRET = "test_client_secret"
        
    def tearDown(self):
        # Clean up
        self.patcher.stop()
        
    @patch('msal.ConfidentialClientApplication')
    def test_authenticate_no_token(self, mock_msal):
        # Mock MSAL app
        mock_app = MagicMock()
        mock_app.get_accounts.return_value = []
        mock_flow = {"user_code": "12345", "verification_uri": "https://example.com"}
        mock_app.initiate_device_flow.return_value = mock_flow
        mock_app.acquire_token_by_device_flow.return_value = {"access_token": "test_token"}
        mock_msal.return_value = mock_app
        
        # Mock token cache
        mock_cache = MagicMock()
        with patch('src.ingestion.cloud.onedrive_client.msal.SerializableTokenCache') as mock_cache_class:
            mock_cache_class.return_value = mock_cache
            
            client = OneDriveClient()
            with patch('builtins.print') as mock_print:
                result = client.authenticate()
                
                self.assertTrue(result)
                self.assertEqual(client.token, {"access_token": "test_token"})
                mock_print.assert_called_once()
                
    @patch('requests.get')
    def test_list_files(self, mock_get):
        # Mock successful API response
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "value": [
                {"name": "file1.txt", "id": "123"},
                {"name": "file2.pdf", "id": "456", "folder": {}}
            ]
        }
        mock_get.return_value = mock_response
        
        client = OneDriveClient()
        client.token = {"access_token": "test_token"}
        
        files = client.list_files("/Documents")
        
        self.assertEqual(len(files), 2)
        self.assertEqual(files[0]["name"], "file1.txt")
        mock_get.assert_called_once()
        
    @patch('requests.get')
    def test_download_file(self, mock_get):
        # Mock successful file download
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.iter_content.return_value = [b"file content"]
        mock_get.return_value = mock_response
        
        client = OneDriveClient()
        client.token = {"access_token": "test_token"}
        
        with tempfile.NamedTemporaryFile() as temp_file:
            result = client.download_file("/Documents/file1.txt", Path(temp_file.name))
            
            self.assertTrue(result)
            mock_get.assert_called_once()

if __name__ == '__main__':
    unittest.main()

================
File: tests/test_processing.py
================
import os
import sys
import unittest
from pathlib import Path
from unittest.mock import patch, MagicMock
import json
import tempfile
import datetime

# Add project root to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.processing.content_processor import ContentProcessor
from src.rag.knowledge_retriever import KnowledgeRetriever

class TestContentProcessor(unittest.TestCase):
    
    def setUp(self):
        # Create test output directory
        self.test_output_dir = tempfile.mkdtemp()
        self.patcher = patch('src.config.config.settings')
        self.mock_settings = self.patcher.start()
        self.mock_settings.OUTPUT_PATH = Path(self.test_output_dir)
        self.mock_settings.SOURCE_RELIABILITY = {
            "moodle": 3,
            "external_research": 2,
            "personal": 1
        }
        
        # Create a test file
        self.test_file = Path(self.test_output_dir) / "test_document.txt"
        with open(self.test_file, 'w', encoding='utf-8') as f:
            f.write("This is a test document for content processing.\n")
            f.write("It contains multiple lines of text.\n")
            f.write("The content processor should extract key points and create a summary.")
        
    def tearDown(self):
        # Clean up
        self.patcher.stop()
        
    def test_init_processor(self):
        # Test processor initialization
        processor = ContentProcessor()
        self.assertEqual(processor.output_dir, Path(self.test_output_dir) / "processed")
        self.assertTrue(processor.output_dir.exists())
        
    @patch('redis.from_url')
    def test_process_text_file(self, mock_redis):
        # Mock Redis
        mock_redis_client = MagicMock()
        mock_redis_client.get.return_value = None
        mock_redis.return_value = mock_redis_client
        
        processor = ContentProcessor()
        content = processor.process_text_file(self.test_file, "moodle")
        
        # Check processed content
        self.assertEqual(content["original_file"], str(self.test_file))
        self.assertEqual(content["source_type"], "moodle")
        self.assertEqual(content["reliability_score"], 3)
        self.assertIn("content", content)
        self.assertIn("key_points", content["content"])
        
        # Check cache was called
        mock_redis_client.setex.assert_called_once()
        
    @patch('redis.from_url')
    def test_process_text_file_with_cache(self, mock_redis):
        # Mock Redis with cached content
        mock_redis_client = MagicMock()
        cached_content = {
            "original_file": str(self.test_file),
            "source_type": "moodle",
            "reliability_score": 3,
            "content": {
                "summary": "Cached summary",
                "key_points": ["Cached point 1", "Cached point 2"]
            }
        }
        mock_redis_client.get.return_value = json.dumps(cached_content).encode('utf-8')
        mock_redis.return_value = mock_redis_client
        
        processor = ContentProcessor()
        content = processor.process_text_file(self.test_file, "moodle")
        
        # Check cached content was returned
        self.assertEqual(content["content"]["summary"], "Cached summary")
        
        # Check cache get was called, but not setex
        mock_redis_client.get.assert_called_once()
        mock_redis_client.setex.assert_not_called()
        
    def test_generate_notes(self):
        # Test note generation
        processor = ContentProcessor()
        
        content = {
            "original_file": str(self.test_file),
            "source_type": "moodle",
            "reliability_score": 3,
            "content": {
                "summary": "Test summary",
                "key_points": ["Point 1", "Point 2", "Point 3"],
                "details": "Test details"
            }
        }
        
        notes_path = processor.generate_notes(content)
        
        # Check the file exists
        self.assertTrue(notes_path.exists())
        
        # Check the content of the file
        with open(notes_path, 'r', encoding='utf-8') as f:
            notes_content = f.read()
            
        self.assertIn("Test summary", notes_content)
        self.assertIn("Point 1", notes_content)
        self.assertIn("Point 2", notes_content)
        self.assertIn("Point 3", notes_content)
        self.assertIn("Test details", notes_content)
        self.assertIn("moodle", notes_content.lower())
        self.assertIn("3", notes_content)  # Reliability score

class TestKnowledgeRetriever(unittest.TestCase):
    
    def setUp(self):
        self.patcher = patch('src.config.config.settings')
        self.mock_settings = self.patcher.start()
        self.mock_settings.PERPLEXITY_API_KEY = "test_perplexity_key"
        self.mock_settings.OPEN_DEEP_RESEARCH_API_KEY = "test_research_key"
        self.mock_settings.SOURCE_RELIABILITY = {
            "moodle": 3,
            "external_research": 2,
            "personal": 1
        }
        
    def tearDown(self):
        self.patcher.stop()
        
    @patch('requests.post')
    def test_query_perplexity(self, mock_post):
        # Mock API response
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "summary": "Perplexity summary",
            "facts": ["Fact 1", "Fact 2"],
            "citations": ["Citation 1", "Citation 2"]
        }
        mock_post.return_value = mock_response
        
        retriever = KnowledgeRetriever()
        result = retriever._query_perplexity("test query")
        
        # Check the result
        self.assertEqual(result["summary"], "Perplexity summary")
        self.assertEqual(result["facts"], ["Fact 1", "Fact 2"])
        
        # Check the API call
        mock_post.assert_called_once()
        args, kwargs = mock_post.call_args
        self.assertEqual(args[0], "https://api.perplexity.ai/search")
        self.assertEqual(kwargs["headers"]["Authorization"], "Bearer test_perplexity_key")
        self.assertEqual(kwargs["json"]["query"], "test query")
        
    @patch('requests.post')
    def test_query_open_deep_research(self, mock_post):
        # Mock API response
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "summary": "Deep research summary",
            "facts": ["Research fact 1", "Research fact 2"],
            "citations": ["Research citation 1"]
        }
        mock_post.return_value = mock_response
        
        retriever = KnowledgeRetriever()
        result = retriever._query_open_deep_research("test query")
        
        # Check the result
        self.assertEqual(result["summary"], "Deep research summary")
        
        # Check the API call
        mock_post.assert_called_once()
        args, kwargs = mock_post.call_args
        self.assertEqual(args[0], "https://api.opendeepresearch.org/v1/search")
        self.assertEqual(kwargs["json"]["query"], "test query")
        
    @patch.object(KnowledgeRetriever, '_query_perplexity')
    def test_enrich_content(self, mock_query):
        # Mock API response
        mock_query.return_value = {
            "summary": "Test external summary",
            "facts": ["External fact 1", "External fact 2"],
            "citations": ["External citation 1"]
        }
        
        # Test content
        content = {
            "original_file": "test_file.txt",
            "source_type": "moodle",
            "reliability_score": 3,
            "content": {
                "summary": "Original summary",
                "key_points": ["Original point 1", "Original point 2"]
            }
        }
        
        retriever = KnowledgeRetriever()
        enriched = retriever.enrich_content(content, "test query")
        
        # Check enriched content
        self.assertEqual(enriched["content"]["summary"], "Original summary")  # Original content preserved
        self.assertIn("external_research", enriched)
        self.assertEqual(len(enriched["external_research"]), 1)
        self.assertEqual(enriched["external_research"][0]["source"], "Perplexity")
        self.assertEqual(enriched["external_research"][0]["reliability_score"], 2)  # From SOURCE_RELIABILITY
        
    def test_generate_source_summary(self):
        # Test content with multiple sources
        content = {
            "original_file": "test_file.txt",
            "source_type": "moodle",
            "external_research": [
                {
                    "source": "Perplexity",
                    "reliability_score": 2,
                    "citations": ["Citation 1", "Citation 2"]
                },
                {
                    "source": "Open Deep Research",
                    "reliability_score": 2,
                    "citations": ["Citation 3"]
                }
            ]
        }
        
        retriever = KnowledgeRetriever()
        summary = retriever.generate_source_summary(content)
        
        # Check summary content
        self.assertIn("Source Summary", summary)
        self.assertIn("test_file.txt", summary)
        self.assertIn("Perplexity", summary)
        self.assertIn("Citation 1", summary)
        self.assertIn("Citation 2", summary)
        self.assertIn("Citation 3", summary)
        
        # Check order (moodle should be first due to higher reliability)
        first_line = summary.split('\n')[3]  # Skip header lines
        self.assertIn("test_file.txt", first_line)

if __name__ == '__main__':
    unittest.main()



================================================================
End of Codebase
================================================================
